{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\L3\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import gym\n",
    "import numpy as np \n",
    "from collections import deque\n",
    "import tensorflow as tf \n",
    "import tflearn as tl \n",
    "from tflearn.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_infos = {'run' : [], 'step' : [], 'total_reward' : [], 'avg_reward' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    sess = tf.InteractiveSession()\n",
    "    a3c_solver = A3CSolver(observation_space, action_space, sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #a3c_solver.model.load_weights('dqn_weights_%i.h5' % (prev_stop))\n",
    "    run = 0\n",
    "    while True:\n",
    "    #for _ in range(300):\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = a3c_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            total_reward += reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            a3c_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next    \n",
    "            if terminal:\n",
    "                \n",
    "                episode_infos['run'].append(run)\n",
    "                episode_infos['step'].append(step)\n",
    "                episode_infos['total_reward'].append(total_reward)\n",
    "                episode_infos['avg_reward'].append(total_reward / step)\n",
    "                \n",
    "                print('epsilon : ' + str(a3c_solver.exploration_rate))\n",
    "                print('run : ' + str(run) + ' score : ' + str(total_reward) + ' avg_score : ' + str(total_reward / step))\n",
    "                \n",
    "                #Check gradients \n",
    "                t_s, t_a, t_r, _, _ = random.sample(a3c_solver.memory, 1)[0]\n",
    "                t_r = np.array(t_r).reshape([1, 1])\n",
    "                t_a = np.identity(2)[t_a].reshape([1, a3c_solver.action_space])\n",
    "                t_loss = a3c_solver.sess.run([a3c_solver.loss], \n",
    "                                                      feed_dict = {a3c_solver.state : t_s, \n",
    "                                                                   a3c_solver.R : t_r, \n",
    "                                                                   a3c_solver.action : t_a})\n",
    "                print('l : ' + str(t_loss))\n",
    "                \n",
    "                break\n",
    "            a3c_solver.experience_replay()\n",
    "        #dqn_solver.model.save_weights('dqn_weights_%i.h5' % (run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function : \n",
    "$$L= L_\\pi + \\alpha L_Q + \\beta L_{reg}$$\n",
    "### Policy loss :\n",
    "$$A = (R - Q_{a_i\\sim \\pi}(a_i,s_i; \\theta'_q))$$\n",
    "$$\\frac{\\partial\\,J(\\pi)}{\\partial\\, \\theta'} = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial\\,log\\,\\pi (a_i|s_i;\\theta')}{\\partial\\,\\theta'}\\; A$$\n",
    "$$\\therefore J(\\pi) = \\frac{1}{n}\\sum_{i=1}^{n}log\\,\\pi (a_i|s_i;\\theta')\\; R\\;\\;\\;\\;\\;\\;[\\because A\\;is\\;considered\\;constant]$$\n",
    "‚àµ We want to maximize ùêΩ(ùúã) $$L_\\pi = -J(\\pi)$$\n",
    "### Value loss :\n",
    "$$J(Q) = \\sum_{i=1}^{n}(R - Q_{a_i\\sim \\pi}(a_i,s_i; \\theta'_q))$$\n",
    "$$L_Q = J(Q)$$\n",
    "### Policy entropy :\n",
    "$$H(\\overrightarrow{\\pi(s)})=-\\sum_{i=1}^{n}\\sum_{k=1}^{m} \\pi(s_i)_k\\cdot log\\, \\pi(s_i)_k$$\n",
    "$$L_{reg}=H(\\overrightarrow{\\pi(s)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0.95\n",
    "l_rate = -0.0001\n",
    "\n",
    "exp_memory_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "exploration_max = 1.0\n",
    "exploration_min = 0.01\n",
    "exploration_decay = 0.9995\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CSolver():\n",
    "    def __init__(self, observation_space, action_space, sess):\n",
    "        self.sess = sess\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen = exp_memory_size)\n",
    "        \n",
    "        self.state = tl.input_data(shape = [None, observation_space], name = 'S_i')\n",
    "        self.action = tl.input_data(shape = [None, action_space], name = 'A_i')\n",
    "        self.R = tl.input_data(shape = [None, 1], name = 'R_i')\n",
    "        \n",
    "        #Actor ùúã(a_i|s_i;ùúÉ‚Ä≤)\n",
    "        self.actor = self.build_actor(self.state, action_space)\n",
    "        #Critic Q_a_i‚àºùúã(a_i,s_i;ùúÉ‚Ä≤_q)\n",
    "        self.critic = self.build_critic(self.state, self.action)\n",
    "        \n",
    "        #Loss L\n",
    "        self.loss = self.compute_loss()\n",
    "        #Trainable vars \n",
    "        t_vars = tf.trainable_variables()\n",
    "        #Optimize ops \n",
    "        self.adam = tf.train.AdamOptimizer(learning_rate = l_rate)\n",
    "        #self.opt = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self.loss)\n",
    "        self.grads = self.adam.compute_gradients(self.loss, t_vars)\n",
    "        self.clipped_grads = [[self.clip_grads(grad), var] for grad, var in self.grads]\n",
    "        self.opt = self.adam.apply_gradients(self.grads)\n",
    "        \n",
    "    def clip_grads(self, values):\n",
    "        if values is None:\n",
    "            return values\n",
    "        return tf.clip_by_value(values, -1.0, 1.0)\n",
    "        \n",
    "    def build_actor(self, state_input, action_space):\n",
    "        with tf.variable_scope('actor', reuse = tf.AUTO_REUSE):\n",
    "            #State input s_i\n",
    "            #a_h1 = tl.fully_connected(state_input, 24)        \n",
    "            #a_h2 = tl.fully_connected(a_h1, 48)        \n",
    "            #a_logit = tl.fully_connected(a_h2, action_space)\n",
    "            a_logit = tl.fully_connected(state_input, action_space)\n",
    "            return softmax(a_logit)\n",
    "    \n",
    "    def build_critic(self, state_input, action_input):\n",
    "        with tf.variable_scope('critic', reuse = tf.AUTO_REUSE):\n",
    "            #Action input a_i\n",
    "            as_h1 = tl.fully_connected(action_input, 24)\n",
    "            as_h2 = tl.fully_connected(as_h1, 48)\n",
    "            #State input s_i\n",
    "            ss_h1 = tl.fully_connected(state_input, 24)\n",
    "            ss_h2 = tl.fully_connected(ss_h1, 48)\n",
    "            #Combine state action input \n",
    "            q_h1 = tl.layers.merge_ops.merge([as_h2, ss_h2], mode = 'elemwise_sum')\n",
    "            return tl.fully_connected(q_h1, 1)\n",
    "    \n",
    "    def remember(self, state, action, reward, state_next, done):\n",
    "        self.memory.append([state, action, reward, state_next, done])\n",
    "        \n",
    "    def act(self, state, infer = False):\n",
    "        #act\n",
    "        if not infer and np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        action = self.sess.run(self.actor, feed_dict = {self.state : state})\n",
    "        return np.argmax(action[0])\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, state_next, done in batch:\n",
    "            R = np.array(reward).reshape([1, 1])\n",
    "            action = np.identity(2)[action].reshape([1, self.action_space])\n",
    "            if not done:\n",
    "                action_next = self.sess.run(self.actor, feed_dict = {self.state : state_next})\n",
    "                R = reward + y * self.sess.run(self.critic, feed_dict = {self.state : state_next, self.action : action_next})\n",
    "            else:\n",
    "                R = np.array(-100).reshape([1, 1]) \n",
    "            #Optimize\n",
    "            loss, _, a, b, c= self.sess.run([self.loss, self.opt, self.L_pi, self.L_v, self.actor], feed_dict = {self.state : state, \n",
    "                                                                        self.action : action, \n",
    "                                                                        self.R : R})\n",
    "            #print(loss, a, b, c)\n",
    "            losses.append(loss)\n",
    "            self.exploration_rate *= exploration_decay\n",
    "            self.exploration_rate = max(exploration_min, self.exploration_rate)\n",
    "\n",
    "    def compute_loss(self):\n",
    "        #Advantage\n",
    "        A = self.R #- self.critic\n",
    "        #Policy loss \n",
    "        self.L_pi = - tf.reduce_mean(tf.reduce_sum(tf.log(self.actor + 1e-13) * tf.stop_gradient(A), axis = -1))\n",
    "        #Value loss\n",
    "        self.L_v = tf.reduce_mean(tf.math.square(self.R - self.critic))\n",
    "        #Policy entropy \n",
    "        self.L_reg = - tf.reduce_mean(tf.reduce_sum(tf.multiply(self.actor, tf.log(self.actor + 1e-13)), axis = -1))\n",
    "        #Total loss \n",
    "        return self.L_pi + alpha * self.L_v #+ beta * self.L_reg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\L3\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epsilon : 1.0\n",
      "run : 1 score : 9.0 avg_score : 0.8181818181818182\n",
      "l : [1.8862686]\n",
      "epsilon : 0.9684988314739038\n",
      "run : 2 score : 21.0 avg_score : 0.9130434782608695\n",
      "l : [1.889775]\n",
      "epsilon : 0.6491392424807709\n",
      "run : 3 score : 24.0 avg_score : 0.9230769230769231\n",
      "l : [1.8637575]\n",
      "epsilon : 0.44210668023652117\n",
      "run : 4 score : 23.0 avg_score : 0.92\n",
      "l : [1.4494327]\n",
      "epsilon : 0.3828009720733851\n",
      "run : 5 score : 8.0 avg_score : 0.8\n",
      "l : [1.6891031]\n",
      "epsilon : 0.3159131022678686\n",
      "run : 6 score : 11.0 avg_score : 0.8461538461538461\n",
      "l : [1.4278322]\n",
      "epsilon : 0.26919261643987724\n",
      "run : 7 score : 9.0 avg_score : 0.8181818181818182\n",
      "l : [4.381146]\n",
      "epsilon : 0.22938163762610433\n",
      "run : 8 score : 9.0 avg_score : 0.8181818181818182\n",
      "l : [3.9553552]\n",
      "epsilon : 0.2018157486181985\n",
      "run : 9 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [5.734805]\n",
      "epsilon : 0.174743491117198\n",
      "run : 10 score : 8.0 avg_score : 0.8\n",
      "l : [1.3947928]\n",
      "epsilon : 0.1489006224876073\n",
      "run : 11 score : 9.0 avg_score : 0.8181818181818182\n",
      "l : [51.440113]\n",
      "epsilon : 0.13100652217870645\n",
      "run : 12 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [19.714754]\n",
      "epsilon : 0.108115391177391\n",
      "run : 13 score : 11.0 avg_score : 0.8461538461538461\n",
      "l : [11.13874]\n",
      "epsilon : 0.09361242136549094\n",
      "run : 14 score : 8.0 avg_score : 0.8\n",
      "l : [106.02662]\n",
      "epsilon : 0.08369130358178066\n",
      "run : 15 score : 6.0 avg_score : 0.75\n",
      "l : [456.84988]\n",
      "epsilon : 0.0736337191589908\n",
      "run : 16 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [550.064]\n",
      "epsilon : 0.0658299598931473\n",
      "run : 17 score : 6.0 avg_score : 0.75\n",
      "l : [172.90079]\n",
      "epsilon : 0.05791885860975983\n",
      "run : 18 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [408.92587]\n",
      "epsilon : 0.05014942403796214\n",
      "run : 19 score : 8.0 avg_score : 0.8\n",
      "l : [214.81712]\n",
      "epsilon : 0.04205435780272403\n",
      "run : 20 score : 10.0 avg_score : 0.8333333333333334\n",
      "l : [1498.2407]\n",
      "epsilon : 0.03700048439120764\n",
      "run : 21 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [2729.9392]\n",
      "epsilon : 0.03255395913084953\n",
      "run : 22 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [1437.9802]\n",
      "epsilon : 0.02818705927144017\n",
      "run : 23 score : 8.0 avg_score : 0.8\n",
      "l : [1930.081]\n",
      "epsilon : 0.024018466409130017\n",
      "run : 24 score : 9.0 avg_score : 0.8181818181818182\n",
      "l : [5755.33]\n",
      "epsilon : 0.021132052369949545\n",
      "run : 25 score : 7.0 avg_score : 0.7777777777777778\n",
      "l : [1536.3727]\n",
      "epsilon : 0.01800682664749925\n",
      "run : 26 score : 9.0 avg_score : 0.8181818181818182\n",
      "l : [8463.984]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e23913c48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-85a02e9257b3>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0ma3c_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;31m#dqn_solver.model.save_weights('dqn_weights_%i.h5' % (run))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-5726310319d0>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m             loss, _, a, b, c= self.sess.run([self.loss, self.opt, self.L_pi, self.L_v, self.actor], feed_dict = {self.state : state, \n\u001b[0;32m     78\u001b[0m                                                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                                                                         self.R : R})\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;31m#print(loss, a, b, c)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8866302,\n",
       " 1.8864217,\n",
       " 1.8859949,\n",
       " 1.8856814,\n",
       " 1.8850236,\n",
       " 1.8850944,\n",
       " 1.8842952,\n",
       " 1.8840072,\n",
       " 1.8835546,\n",
       " 1.8834795,\n",
       " 1.8832691,\n",
       " 1.8830943,\n",
       " 1.8830812,\n",
       " 1.8822014,\n",
       " 1.8818552,\n",
       " 1.8807997,\n",
       " 1.8814535,\n",
       " 4860.7046,\n",
       " 1.8804206,\n",
       " 1.8810799,\n",
       " 1.8804361,\n",
       " 1.8805481,\n",
       " 1.8807585,\n",
       " 1.8812937,\n",
       " 1.8817184,\n",
       " 1.8809578,\n",
       " 1.8811824,\n",
       " 1.8819249,\n",
       " 1.8813486,\n",
       " 1.881192,\n",
       " 1.8816605,\n",
       " 1.8812704,\n",
       " 1.8823018,\n",
       " 1.8813815,\n",
       " 1.8813691,\n",
       " 1.881762,\n",
       " 1.8813431,\n",
       " 1.8815612,\n",
       " 1.8821712,\n",
       " 1.8815744,\n",
       " 1.8815045,\n",
       " 1.8817811,\n",
       " 1.8813827,\n",
       " 1.8813651,\n",
       " 1.8821554,\n",
       " 1.8822999,\n",
       " 1.8813324,\n",
       " 1.8823237,\n",
       " 1.882097,\n",
       " 1.8825989,\n",
       " 1.881489,\n",
       " 1.881233,\n",
       " 1.8812644,\n",
       " 1.8820438,\n",
       " 4860.842,\n",
       " 1.8813679,\n",
       " 1.8818722,\n",
       " 1.8824522,\n",
       " 1.8817766,\n",
       " 1.8827128,\n",
       " 1.88201,\n",
       " 1.8829978,\n",
       " 1.8830905,\n",
       " 1.883127,\n",
       " 4861.015,\n",
       " 1.8824899,\n",
       " 1.8835385,\n",
       " 1.8837781,\n",
       " 1.8833675,\n",
       " 1.8831884,\n",
       " 1.8842168,\n",
       " 1.8833916,\n",
       " 1.8834312,\n",
       " 1.8835642,\n",
       " 1.8836136,\n",
       " 1.8845098,\n",
       " 1.8837028,\n",
       " 1.8837726,\n",
       " 1.8838015,\n",
       " 1.8839769,\n",
       " 1.8840157,\n",
       " 1.8846996,\n",
       " 1.8847659,\n",
       " 1.8848145,\n",
       " 1.8840655,\n",
       " 1.8839785,\n",
       " 1.8848882,\n",
       " 1.8841711,\n",
       " 1.8848195,\n",
       " 1.8856046,\n",
       " 1.8840121,\n",
       " 4861.132,\n",
       " 1.8851392,\n",
       " 1.8843956,\n",
       " 1.8844515,\n",
       " 1.8857156,\n",
       " 1.8845571,\n",
       " 1.884723,\n",
       " 1.8847806,\n",
       " 1.8858768,\n",
       " 4861.2314,\n",
       " 1.8851018,\n",
       " 1.8864291,\n",
       " 1.8856683,\n",
       " 1.886485,\n",
       " 1.8857867,\n",
       " 1.8865678,\n",
       " 1.8870598,\n",
       " 1.8862576,\n",
       " 1.8871446,\n",
       " 1.886881,\n",
       " 1.887232,\n",
       " 1.8870084,\n",
       " 1.8867303,\n",
       " 1.8877691,\n",
       " 1.8872306,\n",
       " 1.8875493,\n",
       " 1.8864491,\n",
       " 1.8866166,\n",
       " 1.887216,\n",
       " 1.8863487,\n",
       " 1.8872454,\n",
       " 1.8865753,\n",
       " 1.8864264,\n",
       " 1.8872578,\n",
       " 1.8863735,\n",
       " 4861.445,\n",
       " 1.886788,\n",
       " 1.8868484,\n",
       " 1.8870585,\n",
       " 1.8868754,\n",
       " 1.8870132,\n",
       " 1.8879592,\n",
       " 1.8885412,\n",
       " 1.8870908,\n",
       " 1.8874766,\n",
       " 4861.542,\n",
       " 1.8874023,\n",
       " 1.8891083,\n",
       " 1.8879789,\n",
       " 1.8890855,\n",
       " 1.8891786,\n",
       " 1.8901048,\n",
       " 1.8909329,\n",
       " 1.889312,\n",
       " 1.8902385,\n",
       " 1.8890767,\n",
       " 1.8889695,\n",
       " 4861.661,\n",
       " 1.8897567,\n",
       " 1.8907626,\n",
       " 1.8897365,\n",
       " 1.8896413,\n",
       " 1.8894935,\n",
       " 1.8892343,\n",
       " 1.8891867,\n",
       " 1.8906326,\n",
       " 1.8914688,\n",
       " 1.8917711,\n",
       " 1.8902254,\n",
       " 1.8902025,\n",
       " 1.8907503,\n",
       " 1.8921742,\n",
       " 1.891524,\n",
       " 1.8933139,\n",
       " 1.8904381,\n",
       " 4861.8447,\n",
       " 1.8927433,\n",
       " 1.8908851,\n",
       " 1.8902292,\n",
       " 1.8903408,\n",
       " 1.8927671,\n",
       " 1.8907355,\n",
       " 1.8932531,\n",
       " 1.8926108,\n",
       " 1.8921694,\n",
       " 1.8905082,\n",
       " 1.8921663,\n",
       " 1.8914417,\n",
       " 1.8932567,\n",
       " 1.891957,\n",
       " 4862.0063,\n",
       " 1.8921888,\n",
       " 1.8912822,\n",
       " 1.8943532,\n",
       " 1.8946104,\n",
       " 1.8933614,\n",
       " 1.8921226,\n",
       " 1.8931482,\n",
       " 1.8924634,\n",
       " 1.8927629,\n",
       " 1.8934677,\n",
       " 1.8937439,\n",
       " 1.8929332,\n",
       " 1.8921657,\n",
       " 1.8940244,\n",
       " 1.8942885,\n",
       " 1.8934851,\n",
       " 4862.1797,\n",
       " 1.8924711,\n",
       " 1.8934097,\n",
       " 1.8941848,\n",
       " 1.8982307,\n",
       " 1.8944737,\n",
       " 1.8951229,\n",
       " 1.8962953,\n",
       " 1.8939668,\n",
       " 1.8947836,\n",
       " 1.8954465,\n",
       " 1.8939364,\n",
       " 1.8979614,\n",
       " 1.8955212,\n",
       " 1.8975141,\n",
       " 4862.378,\n",
       " 1.8948412,\n",
       " 1.8969367,\n",
       " 1.8956385,\n",
       " 1.8966045,\n",
       " 1.8989882,\n",
       " 1.8959463,\n",
       " 1.8989381,\n",
       " 1.8984611,\n",
       " 1.8940135,\n",
       " 1.8969256,\n",
       " 1.8972924,\n",
       " 1.8966112,\n",
       " 1.897392,\n",
       " 1.8949642,\n",
       " 1.896652,\n",
       " 1.899681,\n",
       " 1.8969847,\n",
       " 1.8968956,\n",
       " 1.8961504,\n",
       " 1.8949974,\n",
       " 1.8970068,\n",
       " 4862.581,\n",
       " 1.8993049,\n",
       " 1.8978837,\n",
       " 1.8964921,\n",
       " 1.9011834,\n",
       " 1.8957381,\n",
       " 1.9012026,\n",
       " 1.9010731,\n",
       " 1.8989387,\n",
       " 1.897583,\n",
       " 1.8975067,\n",
       " 1.8997514,\n",
       " 1.901336,\n",
       " 1.901073,\n",
       " 4862.8237,\n",
       " 1.8992076,\n",
       " 1.902623,\n",
       " 1.8990469,\n",
       " 1.8977176,\n",
       " 1.899357,\n",
       " 1.9062879,\n",
       " 1.9004924,\n",
       " 1.8982286,\n",
       " 1.9049098,\n",
       " 1.9004858,\n",
       " 4863.058,\n",
       " 1.8992727,\n",
       " 1.901732,\n",
       " 1.8994474,\n",
       " 1.901437,\n",
       " 1.9049168,\n",
       " 1.9050491,\n",
       " 1.8965933,\n",
       " 1.9044325,\n",
       " 1.9024765,\n",
       " 1.9031137,\n",
       " 1.8994752,\n",
       " 1.9033313,\n",
       " 1.9071922,\n",
       " 1.9077461,\n",
       " 1.902513,\n",
       " 1.9073395,\n",
       " 1.9031849,\n",
       " 1.902029,\n",
       " 1.906853,\n",
       " 1.907207,\n",
       " 1.9043831,\n",
       " 4863.3203,\n",
       " 1.9039124,\n",
       " 1.9018413,\n",
       " 1.9014244,\n",
       " 1.9032114,\n",
       " 1.9006398,\n",
       " 1.9055495,\n",
       " 1.9043492,\n",
       " 1.900926,\n",
       " 1.9049618,\n",
       " 1.9098424,\n",
       " 1.8994468,\n",
       " 1.9011394,\n",
       " 1.9057075,\n",
       " 1.9046211,\n",
       " 1.910367,\n",
       " 4863.583,\n",
       " 1.903548,\n",
       " 1.9099263,\n",
       " 1.9034877,\n",
       " 1.9090865,\n",
       " 1.8985039,\n",
       " 1.9114401,\n",
       " 1.9057825,\n",
       " 1.9027995,\n",
       " 1.904024,\n",
       " 1.9130635,\n",
       " 1.909076,\n",
       " 1.906117,\n",
       " 1.9083482,\n",
       " 1.9129734,\n",
       " 1.9075626,\n",
       " 1.9062417,\n",
       " 1.9059265,\n",
       " 1.9046661,\n",
       " 1.908401,\n",
       " 4864.037,\n",
       " 1.904619,\n",
       " 1.9004316,\n",
       " 1.9051352,\n",
       " 1.9070928,\n",
       " 4864.0776,\n",
       " 1.914475,\n",
       " 4864.346,\n",
       " 1.9118285,\n",
       " 1.9097042,\n",
       " 1.9154792,\n",
       " 1.9198864,\n",
       " 1.9223417,\n",
       " 1.9161601,\n",
       " 1.9080751,\n",
       " 1.9081992,\n",
       " 1.9153888,\n",
       " 1.9147532,\n",
       " 1.9031936,\n",
       " 1.9027417,\n",
       " 1.9162483,\n",
       " 1.9117337,\n",
       " 1.9248769,\n",
       " 1.9129717,\n",
       " 1.9165381,\n",
       " 1.9236944,\n",
       " 1.9168004,\n",
       " 1.9146755,\n",
       " 1.931523,\n",
       " 1.9164509,\n",
       " 1.923493,\n",
       " 1.9207551,\n",
       " 1.9110796,\n",
       " 1.9250344,\n",
       " 1.917264,\n",
       " 1.9034257,\n",
       " 1.9170468,\n",
       " 4865.221,\n",
       " 1.9049456,\n",
       " 1.911058,\n",
       " 1.9274545,\n",
       " 1.91045,\n",
       " 1.9107935,\n",
       " 1.915374,\n",
       " 1.9211044,\n",
       " 1.9254215,\n",
       " 1.9111967,\n",
       " 1.9275132,\n",
       " 1.9271431,\n",
       " 1.919618,\n",
       " 1.9076518,\n",
       " 1.9190295,\n",
       " 1.9241335,\n",
       " 4865.401,\n",
       " 1.9202046,\n",
       " 1.9204668,\n",
       " 1.9146607,\n",
       " 1.9244138,\n",
       " 1.9217579,\n",
       " 1.9231136,\n",
       " 1.9405692,\n",
       " 1.9147797,\n",
       " 1.9218305,\n",
       " 1.9328172,\n",
       " 1.9329203,\n",
       " 1.9239961,\n",
       " 1.9333072,\n",
       " 1.9200125,\n",
       " 1.933577,\n",
       " 1.9335418,\n",
       " 1.9133635,\n",
       " 1.9241738,\n",
       " 1.9070921,\n",
       " 1.9242057,\n",
       " 1.9231421,\n",
       " 1.9180338,\n",
       " 1.9226351,\n",
       " 1.9193199,\n",
       " 1.9277936,\n",
       " 1.913319,\n",
       " 1.9368215,\n",
       " 1.9190409,\n",
       " 1.9232894,\n",
       " 1.9328957,\n",
       " 1.9233788,\n",
       " 4866.1733,\n",
       " 1.9214987,\n",
       " 1.9161111,\n",
       " 1.9263229,\n",
       " 4866.075,\n",
       " 1.9144343,\n",
       " 1.9261184,\n",
       " 1.9396169,\n",
       " 1.9194665,\n",
       " 1.9055414,\n",
       " 1.9377434,\n",
       " 1.9370499,\n",
       " 1.9287585,\n",
       " 1.9125041,\n",
       " 1.9395577,\n",
       " 1.9304608,\n",
       " 1.9350581,\n",
       " 1.91628,\n",
       " 1.9428071,\n",
       " 1.9298404,\n",
       " 1.9177908,\n",
       " 1.9299998,\n",
       " 1.9339015,\n",
       " 1.9409212,\n",
       " 1.9330304,\n",
       " 1.945187,\n",
       " 1.9095948,\n",
       " 1.9306157,\n",
       " 1.9550877,\n",
       " 1.9261185,\n",
       " 1.920388,\n",
       " 1.9119498,\n",
       " 4866.8804,\n",
       " 1.9211329,\n",
       " 1.9488944,\n",
       " 1.9417253,\n",
       " 4867.496,\n",
       " 1.9317515,\n",
       " 1.9272838,\n",
       " 1.9267515,\n",
       " 1.9349556,\n",
       " 1.9490949,\n",
       " 1.9332365,\n",
       " 1.9430552,\n",
       " 1.9215007,\n",
       " 1.9319999,\n",
       " 1.9306962,\n",
       " 1.9273736,\n",
       " 1.9375737,\n",
       " 1.9352472,\n",
       " 1.9371526,\n",
       " 1.938243,\n",
       " 1.9300582,\n",
       " 1.9310855,\n",
       " 1.9094319,\n",
       " 1.9536195,\n",
       " 1.9571667,\n",
       " 1.9256283,\n",
       " 1.9420369,\n",
       " 1.9370021,\n",
       " 1.9267324,\n",
       " 1.9698023,\n",
       " 1.922356,\n",
       " 1.939983,\n",
       " 1.9458809,\n",
       " 1.9461141,\n",
       " 1.9527519,\n",
       " 1.9223878,\n",
       " 1.9206047,\n",
       " 4868.061,\n",
       " 1.9563999,\n",
       " 1.9076142,\n",
       " 1.9410733,\n",
       " 1.9541664,\n",
       " 1.9401408,\n",
       " 1.9262571,\n",
       " 1.9319454,\n",
       " 1.9237945,\n",
       " 1.9418082,\n",
       " 1.9494692,\n",
       " 1.9431415,\n",
       " 1.928206,\n",
       " 4868.576,\n",
       " 1.9511588,\n",
       " 1.9483846,\n",
       " 1.9632715,\n",
       " 1.9240394,\n",
       " 1.9288371,\n",
       " 1.9351153,\n",
       " 1.9636757,\n",
       " 1.9458435,\n",
       " 1.9460957,\n",
       " 1.9426934,\n",
       " 1.9723973,\n",
       " 1.9443785,\n",
       " 1.9107754,\n",
       " 1.9455264,\n",
       " 1.9453175,\n",
       " 1.9678802,\n",
       " 1.9622564,\n",
       " 1.9854212,\n",
       " 1.9270403,\n",
       " 1.9394364,\n",
       " 1.9387016,\n",
       " 1.9482754,\n",
       " 1.9506214,\n",
       " 1.9360056,\n",
       " 1.9083579,\n",
       " 1.9397324,\n",
       " 1.9081999,\n",
       " 1.9477668,\n",
       " 1.9478507,\n",
       " 1.926422,\n",
       " 1.9378401,\n",
       " 1.9478902,\n",
       " 1.9471512,\n",
       " 1.9352639,\n",
       " 1.9325631,\n",
       " 1.9453459,\n",
       " 1.9856274,\n",
       " 1.9301629,\n",
       " 1.9737284,\n",
       " 1.9618149,\n",
       " 1.9541212,\n",
       " 1.9415796,\n",
       " 1.9659843,\n",
       " 1.9471805,\n",
       " 1.9230287,\n",
       " 1.9120777,\n",
       " 1.938727,\n",
       " 1.924472,\n",
       " 1.9378638,\n",
       " 1.9674077,\n",
       " 1.9674311,\n",
       " 1.9557258,\n",
       " 1.9492669,\n",
       " 1.9415201,\n",
       " 1.907267,\n",
       " 1.9329611,\n",
       " 1.9447571,\n",
       " 1.9462078,\n",
       " 1.9146353,\n",
       " 1.9840289,\n",
       " 1.9648641,\n",
       " 1.9421835,\n",
       " 1.9277525,\n",
       " 1.943476,\n",
       " 1.9261037,\n",
       " 1.9606403,\n",
       " 1.9384089,\n",
       " 1.9490739,\n",
       " 1.9287947,\n",
       " 1.9311268,\n",
       " 1.9220898,\n",
       " 1.9454212,\n",
       " 1.9427116,\n",
       " 1.9465324,\n",
       " 1.9389161,\n",
       " 1.9306072,\n",
       " 1.9679482,\n",
       " 1.9541302,\n",
       " 1.9654453,\n",
       " 1.9296187,\n",
       " 1.9453108,\n",
       " 1.9653232,\n",
       " 1.9133141,\n",
       " 1.9204625,\n",
       " 1.9449123,\n",
       " 1.9650733,\n",
       " 1.94103,\n",
       " 1.9430423,\n",
       " 1.936588,\n",
       " 1.9702088,\n",
       " 1.9229655,\n",
       " 1.9356964,\n",
       " 1.9442592,\n",
       " 1.9427311,\n",
       " 1.9698805,\n",
       " 1.9315585,\n",
       " 1.92886,\n",
       " 1.9342074,\n",
       " 1.9180936,\n",
       " 1.962302,\n",
       " 1.916096,\n",
       " 4869.016,\n",
       " 1.9273663,\n",
       " 1.9620597,\n",
       " 1.9370682,\n",
       " 1.9668107,\n",
       " 1.9370748,\n",
       " 1.9558713,\n",
       " 1.9398646,\n",
       " 1.9061717,\n",
       " 1.9544019,\n",
       " 1.9494745,\n",
       " 1.9293103,\n",
       " 1.9462687,\n",
       " 1.9249887,\n",
       " 1.9299674,\n",
       " 1.9419978,\n",
       " 1.9119018,\n",
       " 1.935736,\n",
       " 1.9424396,\n",
       " 1.9404693,\n",
       " 1.9730319,\n",
       " 1.9466956,\n",
       " 1.9705234,\n",
       " 1.9175144,\n",
       " 1.9581838,\n",
       " 1.900074,\n",
       " 1.9114907,\n",
       " 1.9016318,\n",
       " 1.9685479,\n",
       " 1.9764667,\n",
       " 1.919356,\n",
       " 1.9398627,\n",
       " 1.954868,\n",
       " 1.9495186,\n",
       " 1.9377615,\n",
       " 1.9703653,\n",
       " 1.922458,\n",
       " 1.9048785,\n",
       " 1.9384276,\n",
       " 1.9726002,\n",
       " 1.9568911,\n",
       " 1.9180158,\n",
       " 1.9310017,\n",
       " 1.9375186,\n",
       " 1.9294363,\n",
       " 1.9635096,\n",
       " 1.9317067,\n",
       " 1.947417,\n",
       " 1.9297183,\n",
       " 1.9225934,\n",
       " 1.9172318,\n",
       " 1.9693255,\n",
       " 1.947571,\n",
       " 1.8992537,\n",
       " 1.9629502,\n",
       " 1.9383976,\n",
       " 1.9652741,\n",
       " 1.9215987,\n",
       " 1.9355906,\n",
       " 1.9386117,\n",
       " 1.9299121,\n",
       " 1.9369205,\n",
       " 1.9384537,\n",
       " 1.9253623,\n",
       " 1.9866846,\n",
       " 1.953593,\n",
       " 1.9447112,\n",
       " 1.9060543,\n",
       " 1.896003,\n",
       " 1.9150732,\n",
       " 1.9219118,\n",
       " 1.9460161,\n",
       " 1.9305137,\n",
       " 1.9365938,\n",
       " 1.9676933,\n",
       " 1.9364344,\n",
       " 1.9122486,\n",
       " 1.9179473,\n",
       " 1.9441051,\n",
       " 1.9673977,\n",
       " 1.9652874,\n",
       " 4869.4185,\n",
       " 1.9143627,\n",
       " 1.9400066,\n",
       " 1.9570053,\n",
       " 1.957529,\n",
       " 1.9767716,\n",
       " 1.9309556,\n",
       " 1.9599571,\n",
       " 1.9193562,\n",
       " 1.9560226,\n",
       " 1.9261016,\n",
       " 1.9010684,\n",
       " 1.9154396,\n",
       " 1.9306676,\n",
       " 1.9399574,\n",
       " 1.9398415,\n",
       " 1.934335,\n",
       " 1.939743,\n",
       " 4870.8022,\n",
       " 1.93749,\n",
       " 1.9166045,\n",
       " 1.9701235,\n",
       " 2.0023932,\n",
       " 1.9083976,\n",
       " 1.9489212,\n",
       " 1.9519919,\n",
       " 1.9440827,\n",
       " 1.915738,\n",
       " 1.9072397,\n",
       " 1.9247444,\n",
       " 1.9557202,\n",
       " 1.9442043,\n",
       " 4870.9785,\n",
       " 1.9277552,\n",
       " 1.9348397,\n",
       " 1.9349066,\n",
       " 1.944259,\n",
       " 1.927335,\n",
       " 1.9573369,\n",
       " 1.910705,\n",
       " 1.9894588,\n",
       " 1.9611971,\n",
       " 1.9619231,\n",
       " 1.9518064,\n",
       " 1.9745935,\n",
       " 1.9616601,\n",
       " 1.9701095,\n",
       " 1.921731,\n",
       " 1.9434996,\n",
       " 1.9839691,\n",
       " 1.9740039,\n",
       " 1.926825,\n",
       " 2.0007231,\n",
       " 1.9376512,\n",
       " 1.9406533,\n",
       " 1.9736365,\n",
       " 1.9878739,\n",
       " 1.9214087,\n",
       " 2.0143676,\n",
       " 2.022746,\n",
       " 1.9499359,\n",
       " 1.9964057,\n",
       " 1.979466,\n",
       " 1.901578,\n",
       " 1.9199975,\n",
       " 1.939588,\n",
       " 2.0179303,\n",
       " 1.9927418,\n",
       " 1.9544553,\n",
       " 1.9823029,\n",
       " 1.938606,\n",
       " 1.9450189,\n",
       " 1.9873734,\n",
       " 1.9714646,\n",
       " 1.9372536,\n",
       " 1.9078481,\n",
       " 1.967422,\n",
       " 1.9490169,\n",
       " 1.9349726,\n",
       " 1.9765795,\n",
       " 1.9315858,\n",
       " 1.9150103,\n",
       " 1.917536,\n",
       " 1.9465013,\n",
       " 1.9588543,\n",
       " 1.893443,\n",
       " 1.9437684,\n",
       " 1.961245,\n",
       " 1.9452295,\n",
       " 2.0120583,\n",
       " 1.9686,\n",
       " 1.9881728,\n",
       " 1.9741285,\n",
       " 1.9823711,\n",
       " 1.9555486,\n",
       " 1.9904712,\n",
       " 1.8899825,\n",
       " 1.9467349,\n",
       " 1.958264,\n",
       " 1.9251602,\n",
       " 1.9563745,\n",
       " 1.9156781,\n",
       " 1.9767234,\n",
       " 1.945492,\n",
       " 1.9975028,\n",
       " 1.9846334,\n",
       " 1.9041489,\n",
       " 1.939831,\n",
       " 1.9811046,\n",
       " 1.923781,\n",
       " 1.9418235,\n",
       " 1.9618051,\n",
       " 1.9016057,\n",
       " 1.9468424,\n",
       " 1.9921072,\n",
       " 1.9646662,\n",
       " 1.949109,\n",
       " 1.9320004,\n",
       " 1.9883476,\n",
       " 2.0088325,\n",
       " 1.9348695,\n",
       " 1.917796,\n",
       " 1.9258486,\n",
       " 1.9366813,\n",
       " 1.9716151,\n",
       " 1.9877486,\n",
       " 1.9091876,\n",
       " 1.9329188,\n",
       " 1.9618694,\n",
       " 1.9115964,\n",
       " 4872.433,\n",
       " 1.9593117,\n",
       " 1.959963,\n",
       " 1.9903516,\n",
       " 1.9197937,\n",
       " 1.9530635,\n",
       " 2.0143793,\n",
       " 1.9735749,\n",
       " 1.8998873,\n",
       " 4873.2124,\n",
       " 1.9868064,\n",
       " 1.8855983,\n",
       " 1.942846,\n",
       " 1.9996142,\n",
       " 1.9479206,\n",
       " 1.9311117,\n",
       " 1.9578632,\n",
       " 2.0429282,\n",
       " 1.9160756,\n",
       " 1.9039187,\n",
       " 2.0343008,\n",
       " 2.0055194,\n",
       " 1.9536366,\n",
       " 1.940829,\n",
       " 1.9793801,\n",
       " 1.9134688,\n",
       " 1.9869232,\n",
       " 1.9369423,\n",
       " 1.9451919,\n",
       " 2.0173013,\n",
       " 1.9538659,\n",
       " 1.8871925,\n",
       " 2.0017924,\n",
       " 1.9328831,\n",
       " 1.9925268,\n",
       " 1.9436792,\n",
       " 1.9876827,\n",
       " 1.9162707,\n",
       " 1.9796286,\n",
       " 1.9361295,\n",
       " 1.9720702,\n",
       " 1.9478443,\n",
       " 1.9218253,\n",
       " 1.9657625,\n",
       " 2.0499997,\n",
       " 1.9237294,\n",
       " 1.9523482,\n",
       " 2.0088387,\n",
       " 2.00769,\n",
       " 1.9013816,\n",
       " 1.9260615,\n",
       " 1.9429165,\n",
       " 2.0012138,\n",
       " 1.8985347,\n",
       " 2.0038762,\n",
       " 1.9103353,\n",
       " 1.9300634,\n",
       " 1.9578671,\n",
       " 1.9615173,\n",
       " 1.8906331,\n",
       " 1.9545635,\n",
       " 1.9840817,\n",
       " 1.9360726,\n",
       " 1.9500644,\n",
       " 2.0421371,\n",
       " 1.9435129,\n",
       " 2.006064,\n",
       " 1.9497156,\n",
       " 1.9284966,\n",
       " 1.9750279,\n",
       " 1.9115928,\n",
       " 2.0055504,\n",
       " 1.8825692,\n",
       " 1.9953105,\n",
       " 1.9421619,\n",
       " 1.9690453,\n",
       " 1.9392202,\n",
       " 1.9695656,\n",
       " 2.0144343,\n",
       " 1.9472904,\n",
       " 1.9727514,\n",
       " 2.0046515,\n",
       " 2.0342622,\n",
       " 1.8968734,\n",
       " 1.9882588,\n",
       " 1.9094191,\n",
       " 4874.475,\n",
       " 2.0469518,\n",
       " 1.9583495,\n",
       " 2.0116184,\n",
       " 1.9341792,\n",
       " 2.0058384,\n",
       " 1.9897561,\n",
       " 1.8871137,\n",
       " 2.049154,\n",
       " 1.990231,\n",
       " 1.9413096,\n",
       " 1.9744395,\n",
       " 1.974652,\n",
       " 1.9510666,\n",
       " 1.8969241,\n",
       " 1.9106125,\n",
       " 1.9360237,\n",
       " 1.9606266,\n",
       " 1.9516032,\n",
       " 1.974731,\n",
       " 2.0034041,\n",
       " 1.9416862,\n",
       " 2.0145602,\n",
       " 1.958729,\n",
       " 1.9793714,\n",
       " 2.0079234,\n",
       " 2.0109878,\n",
       " 1.9645909,\n",
       " 1.9100132,\n",
       " 1.9550519,\n",
       " 1.9273324,\n",
       " 1.9445524,\n",
       " 1.9271227,\n",
       " 1.9311274,\n",
       " 2.0248811,\n",
       " 1.9417282,\n",
       " 1.9377167,\n",
       " 1.9422262,\n",
       " 1.9432783,\n",
       " 1.9606266,\n",
       " 1.9907196,\n",
       " 1.9460205,\n",
       " 1.9147629,\n",
       " 2.0136216,\n",
       " 1.9721476,\n",
       " 1.9252104,\n",
       " 1.9477384,\n",
       " 1.975808,\n",
       " 2.0521543,\n",
       " 1.9475936,\n",
       " 1.9753588,\n",
       " 1.9069916,\n",
       " 1.9065933,\n",
       " 1.9543307,\n",
       " 2.0088542,\n",
       " 2.0057142,\n",
       " 1.8920262,\n",
       " 1.9309349,\n",
       " 1.9424433,\n",
       " 1.8741556,\n",
       " 2.0061533,\n",
       " 2.0008118,\n",
       " 1.9429799,\n",
       " 1.9269744,\n",
       " 1.9693334,\n",
       " 1.9449784,\n",
       " 1.9173038,\n",
       " 1.9562451,\n",
       " 1.8898569,\n",
       " 1.9137869,\n",
       " 1.9855,\n",
       " 1.9386133,\n",
       " 1.9719996,\n",
       " 1.9577683,\n",
       " 2.0547523,\n",
       " 2.0143964,\n",
       " 1.9418726,\n",
       " 1.9208815,\n",
       " 1.9711299,\n",
       " 1.9558638,\n",
       " 1.9424412,\n",
       " 1.8879511,\n",
       " 1.9715459,\n",
       " 2.0413356,\n",
       " 1.9023052,\n",
       " 1.9109759,\n",
       " 1.945766,\n",
       " 1.9411818,\n",
       " 1.9543127,\n",
       " 1.9385166,\n",
       " 1.9009856,\n",
       " 1.9476008,\n",
       " 1.9129293,\n",
       " 1.8766559,\n",
       " 4874.015,\n",
       " 1.9520471,\n",
       " 1.929762,\n",
       " 1.8980124,\n",
       " 2.00597,\n",
       " 1.9703853,\n",
       " 1.9243128,\n",
       " 1.9726243,\n",
       " 1.8874943,\n",
       " 1.8983294,\n",
       " 4868.7886,\n",
       " 1.9765102,\n",
       " 1.9953022,\n",
       " 2.0175161,\n",
       " 2.0521774,\n",
       " 1.9288802,\n",
       " 1.9095702,\n",
       " 1.9889532,\n",
       " 1.9367355,\n",
       " 4870.474,\n",
       " 2.0179288,\n",
       " 1.9600834,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_infos['run'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.plot(episode_infos['run'][:300], episode_infos['total_reward'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
