{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = 4\n",
    "act_space = env.action_space.n\n",
    "\n",
    "max_eps = 1000\n",
    "l_rate = 0.01\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_norm_rewards(ep_rewards):\n",
    "    disc_ep_rewards = np.zeros_like(ep_rewards)\n",
    "    cum = 0.0\n",
    "    for i in reversed(range(len(ep_rewards))):\n",
    "        cum = ep_rewards[i] + gamma * cum\n",
    "        disc_ep_rewards[i] = cum\n",
    "    \n",
    "    mean = np.mean(disc_ep_rewards)\n",
    "    std = np.std(disc_ep_rewards)\n",
    "    disc_ep_rewards = (disc_ep_rewards - mean) / std\n",
    "    \n",
    "    return disc_ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-8fae72712a90>:7: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From E:\\Users\\L3\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From E:\\Users\\L3\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "state_in = tf.placeholder(tf.float32, [None, obs_space], name = 'st_in')\n",
    "action_in = tf.placeholder(tf.float32, [None, act_space], name = 'ac_in')\n",
    "disc_ep_rewards_in = tf.placeholder(tf.float32, [None,], name = 'disc_r_in')\n",
    "\n",
    "#Policy\n",
    "\n",
    "fc1 = tf.layers.dense(state_in, 10, activation = tf.nn.relu, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "fc2 = tf.layers.dense(fc1, act_space, activation = tf.nn.relu, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "fc3 = tf.layers.dense(fc2, act_space, activation = None, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "'''\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.contrib.layers.fully_connected(inputs = state_in,\n",
    "                                            num_outputs = 10,\n",
    "                                            activation_fn=tf.nn.relu,\n",
    "                                            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "with tf.name_scope(\"fc2\"):\n",
    "    fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                            num_outputs = act_space,\n",
    "                                            activation_fn= tf.nn.relu,\n",
    "                                            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "with tf.name_scope(\"fc3\"):\n",
    "    fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                            num_outputs = act_space,\n",
    "                                            activation_fn= None,\n",
    "                                            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "'''\n",
    "\n",
    "action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "#Loss \n",
    "neg_log_probs = - tf.reduce_sum(tf.math.multiply(action_in, tf.log(action_distribution)), axis = -1)\n",
    "loss = tf.reduce_mean(neg_log_probs * disc_ep_rewards_in)\n",
    "\n",
    "#Optimizer \n",
    "opt = tf.train.AdamOptimizer(l_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  17.0\n",
      "Mean Reward 17.0\n",
      "Max reward so far:  17.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  24.0\n",
      "Mean Reward 20.5\n",
      "Max reward so far:  24.0\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  11.0\n",
      "Mean Reward 17.333333333333332\n",
      "Max reward so far:  24.0\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  9.0\n",
      "Mean Reward 15.25\n",
      "Max reward so far:  24.0\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  15.0\n",
      "Mean Reward 15.2\n",
      "Max reward so far:  24.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  10.0\n",
      "Mean Reward 14.333333333333334\n",
      "Max reward so far:  24.0\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  27.0\n",
      "Mean Reward 16.142857142857142\n",
      "Max reward so far:  27.0\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  12.0\n",
      "Mean Reward 15.625\n",
      "Max reward so far:  27.0\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  60.0\n",
      "Mean Reward 20.555555555555557\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  47.0\n",
      "Mean Reward 23.2\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  12.0\n",
      "Mean Reward 22.181818181818183\n",
      "Max reward so far:  60.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  14.0\n",
      "Mean Reward 21.5\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  19.0\n",
      "Mean Reward 21.307692307692307\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  22.0\n",
      "Mean Reward 21.357142857142858\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  21.0\n",
      "Mean Reward 21.333333333333332\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  15.0\n",
      "Mean Reward 20.9375\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  13.0\n",
      "Mean Reward 20.470588235294116\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  30.0\n",
      "Mean Reward 21.0\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  18.0\n",
      "Mean Reward 20.842105263157894\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  12.0\n",
      "Mean Reward 20.4\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  29.0\n",
      "Mean Reward 20.80952380952381\n",
      "Max reward so far:  60.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  20.0\n",
      "Mean Reward 20.772727272727273\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  13.0\n",
      "Mean Reward 20.434782608695652\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  19.0\n",
      "Mean Reward 20.375\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  9.0\n",
      "Mean Reward 19.92\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  22.0\n",
      "Mean Reward 20.0\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  22.0\n",
      "Mean Reward 20.074074074074073\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  18.0\n",
      "Mean Reward 20.0\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  29.0\n",
      "Mean Reward 20.310344827586206\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  19.0\n",
      "Mean Reward 20.266666666666666\n",
      "Max reward so far:  60.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  73.0\n",
      "Mean Reward 21.967741935483872\n",
      "Max reward so far:  73.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  11.0\n",
      "Mean Reward 21.625\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  50.0\n",
      "Mean Reward 22.484848484848484\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  24.0\n",
      "Mean Reward 22.529411764705884\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  23.0\n",
      "Mean Reward 22.542857142857144\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  24.0\n",
      "Mean Reward 22.583333333333332\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  16.0\n",
      "Mean Reward 22.405405405405407\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  21.0\n",
      "Mean Reward 22.36842105263158\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  27.0\n",
      "Mean Reward 22.487179487179485\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  30.0\n",
      "Mean Reward 22.675\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  17.0\n",
      "Mean Reward 22.536585365853657\n",
      "Max reward so far:  73.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  37.0\n",
      "Mean Reward 22.88095238095238\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  23.0\n",
      "Mean Reward 22.88372093023256\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  11.0\n",
      "Mean Reward 22.613636363636363\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  42.0\n",
      "Mean Reward 23.044444444444444\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  12.0\n",
      "Mean Reward 22.804347826086957\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  22.0\n",
      "Mean Reward 22.78723404255319\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  46.0\n",
      "Mean Reward 23.270833333333332\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  14.0\n",
      "Mean Reward 23.081632653061224\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  10.0\n",
      "Mean Reward 22.82\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  21.0\n",
      "Mean Reward 22.784313725490197\n",
      "Max reward so far:  73.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  22.0\n",
      "Mean Reward 22.76923076923077\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  24.0\n",
      "Mean Reward 22.79245283018868\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  24.0\n",
      "Mean Reward 22.814814814814813\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  24.0\n",
      "Mean Reward 22.836363636363636\n",
      "Max reward so far:  73.0\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  81.0\n",
      "Mean Reward 23.875\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  28.0\n",
      "Mean Reward 23.94736842105263\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  18.0\n",
      "Mean Reward 23.844827586206897\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  26.0\n",
      "Mean Reward 23.88135593220339\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  12.0\n",
      "Mean Reward 23.683333333333334\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  52.0\n",
      "Mean Reward 24.147540983606557\n",
      "Max reward so far:  81.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  12.0\n",
      "Mean Reward 23.951612903225808\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  32.0\n",
      "Mean Reward 24.07936507936508\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  23.0\n",
      "Mean Reward 24.0625\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  23.0\n",
      "Mean Reward 24.046153846153846\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  25.0\n",
      "Mean Reward 24.060606060606062\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  18.0\n",
      "Mean Reward 23.970149253731343\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  19.0\n",
      "Mean Reward 23.897058823529413\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  23.0\n",
      "Mean Reward 23.884057971014492\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  9.0\n",
      "Mean Reward 23.67142857142857\n",
      "Max reward so far:  81.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  19.0\n",
      "Mean Reward 23.6056338028169\n",
      "Max reward so far:  81.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  31.0\n",
      "Mean Reward 23.708333333333332\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  48.0\n",
      "Mean Reward 24.041095890410958\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  25.0\n",
      "Mean Reward 24.054054054054053\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  29.0\n",
      "Mean Reward 24.12\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  11.0\n",
      "Mean Reward 23.94736842105263\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  19.0\n",
      "Mean Reward 23.883116883116884\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  49.0\n",
      "Mean Reward 24.205128205128204\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  19.0\n",
      "Mean Reward 24.139240506329113\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  14.0\n",
      "Mean Reward 24.0125\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  16.0\n",
      "Mean Reward 23.91358024691358\n",
      "Max reward so far:  81.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  16.0\n",
      "Mean Reward 23.817073170731707\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  20.0\n",
      "Mean Reward 23.771084337349397\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  30.0\n",
      "Mean Reward 23.845238095238095\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  29.0\n",
      "Mean Reward 23.905882352941177\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  33.0\n",
      "Mean Reward 24.011627906976745\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  16.0\n",
      "Mean Reward 23.919540229885058\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  18.0\n",
      "Mean Reward 23.852272727272727\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  17.0\n",
      "Mean Reward 23.775280898876403\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  12.0\n",
      "Mean Reward 23.644444444444446\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  24.0\n",
      "Mean Reward 23.64835164835165\n",
      "Max reward so far:  81.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  21.0\n",
      "Mean Reward 23.619565217391305\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  12.0\n",
      "Mean Reward 23.49462365591398\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  43.0\n",
      "Mean Reward 23.70212765957447\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  43.0\n",
      "Mean Reward 23.905263157894737\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  13.0\n",
      "Mean Reward 23.791666666666668\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  50.0\n",
      "Mean Reward 24.061855670103093\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  25.0\n",
      "Mean Reward 24.071428571428573\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  27.0\n",
      "Mean Reward 24.1010101010101\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  34.0\n",
      "Mean Reward 24.2\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  35.0\n",
      "Mean Reward 24.306930693069308\n",
      "Max reward so far:  81.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  28.0\n",
      "Mean Reward 24.34313725490196\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  37.0\n",
      "Mean Reward 24.466019417475728\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  35.0\n",
      "Mean Reward 24.567307692307693\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  38.0\n",
      "Mean Reward 24.695238095238096\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  52.0\n",
      "Mean Reward 24.952830188679247\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  19.0\n",
      "Mean Reward 24.897196261682243\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  11.0\n",
      "Mean Reward 24.76851851851852\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  38.0\n",
      "Mean Reward 24.889908256880734\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  68.0\n",
      "Mean Reward 25.28181818181818\n",
      "Max reward so far:  81.0\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  139.0\n",
      "Mean Reward 26.306306306306308\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  23.0\n",
      "Mean Reward 26.276785714285715\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  13.0\n",
      "Mean Reward 26.15929203539823\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  10.0\n",
      "Mean Reward 26.017543859649123\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  27.0\n",
      "Mean Reward 26.026086956521738\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  45.0\n",
      "Mean Reward 26.189655172413794\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  39.0\n",
      "Mean Reward 26.299145299145298\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  47.0\n",
      "Mean Reward 26.47457627118644\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  21.0\n",
      "Mean Reward 26.428571428571427\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  19.0\n",
      "Mean Reward 26.366666666666667\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  57.0\n",
      "Mean Reward 26.619834710743802\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  14.0\n",
      "Mean Reward 26.516393442622952\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  23.0\n",
      "Mean Reward 26.48780487804878\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  28.0\n",
      "Mean Reward 26.5\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  45.0\n",
      "Mean Reward 26.648\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  24.0\n",
      "Mean Reward 26.626984126984127\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  55.0\n",
      "Mean Reward 26.8503937007874\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  13.0\n",
      "Mean Reward 26.7421875\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  18.0\n",
      "Mean Reward 26.674418604651162\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  29.0\n",
      "Mean Reward 26.692307692307693\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  30.0\n",
      "Mean Reward 26.717557251908396\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  70.0\n",
      "Mean Reward 27.045454545454547\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  11.0\n",
      "Mean Reward 26.924812030075188\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  25.0\n",
      "Mean Reward 26.91044776119403\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  27.0\n",
      "Mean Reward 26.91111111111111\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  39.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  139.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  27.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  11.0\n",
      "Mean Reward 26.884057971014492\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  19.0\n",
      "Mean Reward 26.827338129496404\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  27.0\n",
      "Mean Reward 26.82857142857143\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  126.0\n",
      "Mean Reward 27.53191489361702\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  22.0\n",
      "Mean Reward 27.492957746478872\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  39.0\n",
      "Mean Reward 27.573426573426573\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  11.0\n",
      "Mean Reward 27.458333333333332\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  36.0\n",
      "Mean Reward 27.517241379310345\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  18.0\n",
      "Mean Reward 27.45205479452055\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  17.0\n",
      "Mean Reward 27.38095238095238\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  19.0\n",
      "Mean Reward 27.324324324324323\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  25.0\n",
      "Mean Reward 27.308724832214764\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  11.0\n",
      "Mean Reward 27.2\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  22.0\n",
      "Mean Reward 27.165562913907284\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  17.0\n",
      "Mean Reward 27.098684210526315\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  22.0\n",
      "Mean Reward 27.065359477124183\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  23.0\n",
      "Mean Reward 27.038961038961038\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  56.0\n",
      "Mean Reward 27.225806451612904\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  25.0\n",
      "Mean Reward 27.21153846153846\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  25.0\n",
      "Mean Reward 27.197452229299362\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  50.0\n",
      "Mean Reward 27.341772151898734\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  16.0\n",
      "Mean Reward 27.270440251572328\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  14.0\n",
      "Mean Reward 27.1875\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  10.0\n",
      "Mean Reward 27.080745341614907\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  34.0\n",
      "Mean Reward 27.123456790123456\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  15.0\n",
      "Mean Reward 27.049079754601227\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  38.0\n",
      "Mean Reward 27.115853658536587\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  25.0\n",
      "Mean Reward 27.1030303030303\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  29.0\n",
      "Mean Reward 27.1144578313253\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  47.0\n",
      "Mean Reward 27.233532934131738\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  49.0\n",
      "Mean Reward 27.363095238095237\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  25.0\n",
      "Mean Reward 27.349112426035504\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  19.0\n",
      "Mean Reward 27.3\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  17.0\n",
      "Mean Reward 27.239766081871345\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  50.0\n",
      "Mean Reward 27.372093023255815\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  39.0\n",
      "Mean Reward 27.439306358381504\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  21.0\n",
      "Mean Reward 27.402298850574713\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  36.0\n",
      "Mean Reward 27.451428571428572\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  18.0\n",
      "Mean Reward 27.397727272727273\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  16.0\n",
      "Mean Reward 27.333333333333332\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  73.0\n",
      "Mean Reward 27.589887640449437\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  13.0\n",
      "Mean Reward 27.508379888268156\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  21.0\n",
      "Mean Reward 27.47222222222222\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  129.0\n",
      "Mean Reward 28.03314917127072\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  117.0\n",
      "Mean Reward 28.521978021978022\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  87.0\n",
      "Mean Reward 28.84153005464481\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  34.0\n",
      "Mean Reward 28.869565217391305\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  98.0\n",
      "Mean Reward 29.243243243243242\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  73.0\n",
      "Mean Reward 29.478494623655912\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  27.0\n",
      "Mean Reward 29.46524064171123\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  45.0\n",
      "Mean Reward 29.54787234042553\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  110.0\n",
      "Mean Reward 29.973544973544975\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  48.0\n",
      "Mean Reward 30.068421052631578\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  103.0\n",
      "Mean Reward 30.45026178010471\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  108.0\n",
      "Mean Reward 30.854166666666668\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  91.0\n",
      "Mean Reward 31.16580310880829\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  41.0\n",
      "Mean Reward 31.216494845360824\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  81.0\n",
      "Mean Reward 31.47179487179487\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  97.0\n",
      "Mean Reward 31.806122448979593\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  41.0\n",
      "Mean Reward 31.85279187817259\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  60.0\n",
      "Mean Reward 31.994949494949495\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  21.0\n",
      "Mean Reward 31.939698492462313\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  27.0\n",
      "Mean Reward 31.915\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  84.0\n",
      "Mean Reward 32.17412935323383\n",
      "Max reward so far:  139.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  37.0\n",
      "Mean Reward 32.198019801980195\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  15.0\n",
      "Mean Reward 32.11330049261084\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  14.0\n",
      "Mean Reward 32.02450980392157\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  73.0\n",
      "Mean Reward 32.22439024390244\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  88.0\n",
      "Mean Reward 32.49514563106796\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  124.0\n",
      "Mean Reward 32.93719806763285\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  134.0\n",
      "Mean Reward 33.42307692307692\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  46.0\n",
      "Mean Reward 33.483253588516746\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  125.0\n",
      "Mean Reward 33.91904761904762\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  46.0\n",
      "Mean Reward 33.976303317535546\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  21.0\n",
      "Mean Reward 33.91509433962264\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  36.0\n",
      "Mean Reward 33.924882629107984\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  66.0\n",
      "Mean Reward 34.074766355140184\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  32.0\n",
      "Mean Reward 34.06511627906977\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  55.0\n",
      "Mean Reward 34.16203703703704\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  88.0\n",
      "Mean Reward 34.41013824884793\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  115.0\n",
      "Mean Reward 34.77981651376147\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  30.0\n",
      "Mean Reward 34.757990867579906\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  128.0\n",
      "Mean Reward 35.18181818181818\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  36.0\n",
      "Mean Reward 35.18552036199095\n",
      "Max reward so far:  139.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  127.0\n",
      "Mean Reward 35.5990990990991\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  67.0\n",
      "Mean Reward 35.73991031390135\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  28.0\n",
      "Mean Reward 35.705357142857146\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  129.0\n",
      "Mean Reward 36.12\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  72.0\n",
      "Mean Reward 36.2787610619469\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  126.0\n",
      "Mean Reward 36.67400881057269\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  119.0\n",
      "Mean Reward 37.03508771929825\n",
      "Max reward so far:  139.0\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  168.0\n",
      "Mean Reward 37.60698689956332\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  162.0\n",
      "Mean Reward 38.14782608695652\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  45.0\n",
      "Mean Reward 38.17748917748918\n",
      "Max reward so far:  168.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  19.0\n",
      "Mean Reward 38.0948275862069\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  121.0\n",
      "Mean Reward 38.45064377682404\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  32.0\n",
      "Mean Reward 38.42307692307692\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  51.0\n",
      "Mean Reward 38.47659574468085\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  147.0\n",
      "Mean Reward 38.936440677966104\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  42.0\n",
      "Mean Reward 38.949367088607595\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  24.0\n",
      "Mean Reward 38.88655462184874\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  129.0\n",
      "Mean Reward 39.26359832635983\n",
      "Max reward so far:  168.0\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  217.0\n",
      "Mean Reward 40.00416666666667\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  163.0\n",
      "Mean Reward 40.51452282157676\n",
      "Max reward so far:  217.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  99.0\n",
      "Mean Reward 40.756198347107436\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  135.0\n",
      "Mean Reward 41.1440329218107\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  17.0\n",
      "Mean Reward 41.045081967213115\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  106.0\n",
      "Mean Reward 41.310204081632655\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  116.0\n",
      "Mean Reward 41.613821138211385\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  108.0\n",
      "Mean Reward 41.88259109311741\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  24.0\n",
      "Mean Reward 41.810483870967744\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  56.0\n",
      "Mean Reward 41.86746987951807\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  104.0\n",
      "Mean Reward 42.116\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  138.0\n",
      "Mean Reward 42.49800796812749\n",
      "Max reward so far:  217.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  40.0\n",
      "Mean Reward 42.48809523809524\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  110.0\n",
      "Mean Reward 42.75494071146245\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  68.0\n",
      "Mean Reward 42.854330708661415\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  92.0\n",
      "Mean Reward 43.04705882352941\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  25.0\n",
      "Mean Reward 42.9765625\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  95.0\n",
      "Mean Reward 43.17898832684825\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  159.0\n",
      "Mean Reward 43.627906976744185\n",
      "Max reward so far:  217.0\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  272.0\n",
      "Mean Reward 44.50965250965251\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  223.0\n",
      "Mean Reward 45.19615384615385\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  30.0\n",
      "Mean Reward 45.13793103448276\n",
      "Max reward so far:  272.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  87.0\n",
      "Mean Reward 45.29770992366412\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  60.0\n",
      "Mean Reward 45.35361216730038\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  142.0\n",
      "Mean Reward 45.71969696969697\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  177.0\n",
      "Mean Reward 46.21509433962264\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  25.0\n",
      "Mean Reward 46.13533834586466\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  161.0\n",
      "Mean Reward 46.56554307116105\n",
      "Max reward so far:  272.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  170.0\n",
      "Mean Reward 47.026119402985074\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  225.0\n",
      "Mean Reward 47.687732342007436\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  151.0\n",
      "Mean Reward 48.07037037037037\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  128.0\n",
      "Mean Reward 48.36531365313653\n",
      "Max reward so far:  272.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  209.0\n",
      "Mean Reward 48.955882352941174\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  139.0\n",
      "Mean Reward 49.285714285714285\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  157.0\n",
      "Mean Reward 49.67883211678832\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  18.0\n",
      "Mean Reward 49.56363636363636\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  208.0\n",
      "Mean Reward 50.13768115942029\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  16.0\n",
      "Mean Reward 50.014440433213\n",
      "Max reward so far:  272.0\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  444.0\n",
      "Mean Reward 51.431654676258994\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  198.0\n",
      "Mean Reward 51.956989247311824\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  149.0\n",
      "Mean Reward 52.30357142857143\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  268.0\n",
      "Mean Reward 53.071174377224196\n",
      "Max reward so far:  444.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  228.0\n",
      "Mean Reward 53.691489361702125\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  422.0\n",
      "Mean Reward 54.99293286219081\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  162.0\n",
      "Mean Reward 55.36971830985915\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  168.0\n",
      "Mean Reward 55.76491228070176\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  254.0\n",
      "Mean Reward 56.45804195804196\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  300.0\n",
      "Mean Reward 57.30662020905923\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  105.0\n",
      "Mean Reward 57.47222222222222\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  290.0\n",
      "Mean Reward 58.27681660899654\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  262.0\n",
      "Mean Reward 58.97931034482759\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  183.0\n",
      "Mean Reward 59.40549828178694\n",
      "Max reward so far:  444.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  130.0\n",
      "Mean Reward 59.647260273972606\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  116.0\n",
      "Mean Reward 59.839590443686006\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  152.0\n",
      "Mean Reward 60.1530612244898\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  162.0\n",
      "Mean Reward 60.498305084745766\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  304.0\n",
      "Mean Reward 61.320945945945944\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  201.0\n",
      "Mean Reward 61.79124579124579\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  164.0\n",
      "Mean Reward 62.13422818791946\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  150.0\n",
      "Mean Reward 62.42809364548495\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  194.0\n",
      "Mean Reward 62.86666666666667\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  300\n",
      "Reward:  159.0\n",
      "Mean Reward 63.18604651162791\n",
      "Max reward so far:  444.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  301\n",
      "Reward:  200.0\n",
      "Mean Reward 63.63907284768212\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  302\n",
      "Reward:  173.0\n",
      "Mean Reward 64.0\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  303\n",
      "Reward:  128.0\n",
      "Mean Reward 64.21052631578948\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  304\n",
      "Reward:  188.0\n",
      "Mean Reward 64.61639344262295\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  305\n",
      "Reward:  150.0\n",
      "Mean Reward 64.89542483660131\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  306\n",
      "Reward:  165.0\n",
      "Mean Reward 65.2214983713355\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  307\n",
      "Reward:  188.0\n",
      "Mean Reward 65.62012987012987\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  308\n",
      "Reward:  164.0\n",
      "Mean Reward 65.93851132686085\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  309\n",
      "Reward:  133.0\n",
      "Mean Reward 66.15483870967742\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  310\n",
      "Reward:  147.0\n",
      "Mean Reward 66.41479099678456\n",
      "Max reward so far:  444.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  311\n",
      "Reward:  135.0\n",
      "Mean Reward 66.63461538461539\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  312\n",
      "Reward:  147.0\n",
      "Mean Reward 66.89137380191693\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  313\n",
      "Reward:  162.0\n",
      "Mean Reward 67.19426751592357\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  314\n",
      "Reward:  223.0\n",
      "Mean Reward 67.68888888888888\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  315\n",
      "Reward:  236.0\n",
      "Mean Reward 68.22151898734177\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  316\n",
      "Reward:  166.0\n",
      "Mean Reward 68.52996845425868\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  317\n",
      "Reward:  251.0\n",
      "Mean Reward 69.10377358490567\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  318\n",
      "Reward:  154.0\n",
      "Mean Reward 69.36990595611286\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  319\n",
      "Reward:  261.0\n",
      "Mean Reward 69.96875\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  320\n",
      "Reward:  377.0\n",
      "Mean Reward 70.92523364485982\n",
      "Max reward so far:  444.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  321\n",
      "Reward:  310.0\n",
      "Mean Reward 71.66770186335404\n",
      "Max reward so far:  444.0\n",
      "==========================================\n",
      "Episode:  322\n",
      "Reward:  741.0\n",
      "Mean Reward 73.73993808049535\n",
      "Max reward so far:  741.0\n",
      "==========================================\n",
      "Episode:  323\n",
      "Reward:  390.0\n",
      "Mean Reward 74.71604938271605\n",
      "Max reward so far:  741.0\n",
      "==========================================\n",
      "Episode:  324\n",
      "Reward:  294.0\n",
      "Mean Reward 75.39076923076924\n",
      "Max reward so far:  741.0\n",
      "==========================================\n",
      "Episode:  325\n",
      "Reward:  184.0\n",
      "Mean Reward 75.7239263803681\n",
      "Max reward so far:  741.0\n",
      "==========================================\n",
      "Episode:  326\n",
      "Reward:  394.0\n",
      "Mean Reward 76.69724770642202\n",
      "Max reward so far:  741.0\n",
      "==========================================\n",
      "Episode:  327\n",
      "Reward:  789.0\n",
      "Mean Reward 78.8689024390244\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  328\n",
      "Reward:  220.0\n",
      "Mean Reward 79.29787234042553\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  329\n",
      "Reward:  54.0\n",
      "Mean Reward 79.22121212121212\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  330\n",
      "Reward:  689.0\n",
      "Mean Reward 81.06344410876133\n",
      "Max reward so far:  789.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "==========================================\n",
      "Episode:  331\n",
      "Reward:  313.0\n",
      "Mean Reward 81.76204819277109\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  332\n",
      "Reward:  98.0\n",
      "Mean Reward 81.8108108108108\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  333\n",
      "Reward:  217.0\n",
      "Mean Reward 82.21556886227545\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  334\n",
      "Reward:  696.0\n",
      "Mean Reward 84.04776119402985\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  335\n",
      "Reward:  751.0\n",
      "Mean Reward 86.0327380952381\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  336\n",
      "Reward:  315.0\n",
      "Mean Reward 86.71216617210682\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  337\n",
      "Reward:  213.0\n",
      "Mean Reward 87.08579881656804\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  338\n",
      "Reward:  526.0\n",
      "Mean Reward 88.38053097345133\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  339\n",
      "Reward:  337.0\n",
      "Mean Reward 89.11176470588235\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  340\n",
      "Reward:  396.0\n",
      "Mean Reward 90.0117302052786\n",
      "Max reward so far:  789.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  341\n",
      "Reward:  472.0\n",
      "Mean Reward 91.12865497076024\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  342\n",
      "Reward:  376.0\n",
      "Mean Reward 91.95918367346938\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  343\n",
      "Reward:  146.0\n",
      "Mean Reward 92.11627906976744\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  344\n",
      "Reward:  228.0\n",
      "Mean Reward 92.51014492753623\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  345\n",
      "Reward:  226.0\n",
      "Mean Reward 92.89595375722543\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  346\n",
      "Reward:  149.0\n",
      "Mean Reward 93.05763688760807\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  347\n",
      "Reward:  23.0\n",
      "Mean Reward 92.85632183908046\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  348\n",
      "Reward:  720.0\n",
      "Mean Reward 94.65329512893983\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  349\n",
      "Reward:  605.0\n",
      "Mean Reward 96.11142857142858\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  350\n",
      "Reward:  448.0\n",
      "Mean Reward 97.11396011396012\n",
      "Max reward so far:  789.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  351\n",
      "Reward:  272.0\n",
      "Mean Reward 97.61079545454545\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  352\n",
      "Reward:  540.0\n",
      "Mean Reward 98.86402266288952\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  353\n",
      "Reward:  282.0\n",
      "Mean Reward 99.38135593220339\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  354\n",
      "Reward:  395.0\n",
      "Mean Reward 100.21408450704226\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  355\n",
      "Reward:  373.0\n",
      "Mean Reward 100.98033707865169\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  356\n",
      "Reward:  318.0\n",
      "Mean Reward 101.58823529411765\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  357\n",
      "Reward:  340.0\n",
      "Mean Reward 102.25418994413408\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  358\n",
      "Reward:  295.0\n",
      "Mean Reward 102.79108635097494\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  359\n",
      "Reward:  137.0\n",
      "Mean Reward 102.8861111111111\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  360\n",
      "Reward:  428.0\n",
      "Mean Reward 103.78670360110803\n",
      "Max reward so far:  789.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  361\n",
      "Reward:  575.0\n",
      "Mean Reward 105.08839779005525\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  362\n",
      "Reward:  370.0\n",
      "Mean Reward 105.81818181818181\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  363\n",
      "Reward:  417.0\n",
      "Mean Reward 106.67307692307692\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  364\n",
      "Reward:  367.0\n",
      "Mean Reward 107.38630136986302\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  365\n",
      "Reward:  253.0\n",
      "Mean Reward 107.78415300546447\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  366\n",
      "Reward:  277.0\n",
      "Mean Reward 108.24523160762942\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  367\n",
      "Reward:  402.0\n",
      "Mean Reward 109.04347826086956\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  368\n",
      "Reward:  329.0\n",
      "Mean Reward 109.63956639566396\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  369\n",
      "Reward:  484.0\n",
      "Mean Reward 110.65135135135135\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  370\n",
      "Reward:  146.0\n",
      "Mean Reward 110.7466307277628\n",
      "Max reward so far:  789.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  371\n",
      "Reward:  147.0\n",
      "Mean Reward 110.84408602150538\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  372\n",
      "Reward:  391.0\n",
      "Mean Reward 111.59517426273459\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  373\n",
      "Reward:  218.0\n",
      "Mean Reward 111.87967914438502\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  374\n",
      "Reward:  170.0\n",
      "Mean Reward 112.03466666666667\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  375\n",
      "Reward:  195.0\n",
      "Mean Reward 112.25531914893617\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  376\n",
      "Reward:  247.0\n",
      "Mean Reward 112.61273209549071\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  377\n",
      "Reward:  291.0\n",
      "Mean Reward 113.08465608465609\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  378\n",
      "Reward:  244.0\n",
      "Mean Reward 113.43007915567283\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  379\n",
      "Reward:  225.0\n",
      "Mean Reward 113.72368421052632\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  380\n",
      "Reward:  233.0\n",
      "Mean Reward 114.03674540682415\n",
      "Max reward so far:  789.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  381\n",
      "Reward:  345.0\n",
      "Mean Reward 114.6413612565445\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  382\n",
      "Reward:  326.0\n",
      "Mean Reward 115.19321148825065\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  383\n",
      "Reward:  318.0\n",
      "Mean Reward 115.72135416666667\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  384\n",
      "Reward:  391.0\n",
      "Mean Reward 116.43636363636364\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  385\n",
      "Reward:  591.0\n",
      "Mean Reward 117.66580310880829\n",
      "Max reward so far:  789.0\n",
      "==========================================\n",
      "Episode:  386\n",
      "Reward:  1767.0\n",
      "Mean Reward 121.92764857881137\n",
      "Max reward so far:  1767.0\n",
      "==========================================\n",
      "Episode:  387\n",
      "Reward:  12563.0\n",
      "Mean Reward 153.9922680412371\n",
      "Max reward so far:  12563.0\n",
      "==========================================\n",
      "Episode:  388\n",
      "Reward:  13073.0\n",
      "Mean Reward 187.2030848329049\n",
      "Max reward so far:  13073.0\n",
      "==========================================\n",
      "Episode:  389\n",
      "Reward:  141.0\n",
      "Mean Reward 187.08461538461538\n",
      "Max reward so far:  13073.0\n",
      "==========================================\n",
      "Episode:  390\n",
      "Reward:  24047.0\n",
      "Mean Reward 248.1074168797954\n",
      "Max reward so far:  24047.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  391\n",
      "Reward:  113.0\n",
      "Mean Reward 247.7627551020408\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  392\n",
      "Reward:  150.0\n",
      "Mean Reward 247.51399491094148\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  393\n",
      "Reward:  147.0\n",
      "Mean Reward 247.25888324873097\n",
      "Max reward so far:  24047.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  394\n",
      "Reward:  144.0\n",
      "Mean Reward 246.99746835443037\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  395\n",
      "Reward:  108.0\n",
      "Mean Reward 246.64646464646464\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  396\n",
      "Reward:  106.0\n",
      "Mean Reward 246.29219143576827\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  397\n",
      "Reward:  110.0\n",
      "Mean Reward 245.9497487437186\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  398\n",
      "Reward:  64.0\n",
      "Mean Reward 245.4937343358396\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  399\n",
      "Reward:  33.0\n",
      "Mean Reward 244.9625\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  400\n",
      "Reward:  86.0\n",
      "Mean Reward 244.5660847880299\n",
      "Max reward so far:  24047.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  401\n",
      "Reward:  69.0\n",
      "Mean Reward 244.12935323383084\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  402\n",
      "Reward:  66.0\n",
      "Mean Reward 243.68734491315138\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  403\n",
      "Reward:  45.0\n",
      "Mean Reward 243.19554455445544\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  404\n",
      "Reward:  113.0\n",
      "Mean Reward 242.8740740740741\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  405\n",
      "Reward:  124.0\n",
      "Mean Reward 242.58128078817734\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  406\n",
      "Reward:  144.0\n",
      "Mean Reward 242.33906633906633\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  407\n",
      "Reward:  148.0\n",
      "Mean Reward 242.1078431372549\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  408\n",
      "Reward:  160.0\n",
      "Mean Reward 241.90709046454768\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  409\n",
      "Reward:  246.0\n",
      "Mean Reward 241.9170731707317\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  410\n",
      "Reward:  222.0\n",
      "Mean Reward 241.86861313868613\n",
      "Max reward so far:  24047.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  411\n",
      "Reward:  226.0\n",
      "Mean Reward 241.83009708737865\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  412\n",
      "Reward:  425.0\n",
      "Mean Reward 242.27360774818402\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  413\n",
      "Reward:  478.0\n",
      "Mean Reward 242.84299516908212\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  414\n",
      "Reward:  552.0\n",
      "Mean Reward 243.5879518072289\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  415\n",
      "Reward:  1510.0\n",
      "Mean Reward 246.63221153846155\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  416\n",
      "Reward:  6026.0\n",
      "Mean Reward 260.4916067146283\n",
      "Max reward so far:  24047.0\n",
      "==========================================\n",
      "Episode:  417\n",
      "Reward:  97084.0\n",
      "Mean Reward 492.1267942583732\n",
      "Max reward so far:  97084.0\n",
      "==========================================\n",
      "Episode:  418\n",
      "Reward:  149521.0\n",
      "Mean Reward 847.8042959427207\n",
      "Max reward so far:  149521.0\n",
      "==========================================\n",
      "Episode:  419\n",
      "Reward:  292909.0\n",
      "Mean Reward 1543.1880952380952\n",
      "Max reward so far:  292909.0\n",
      "==========================================\n",
      "Episode:  420\n",
      "Reward:  509186.0\n",
      "Mean Reward 2748.9904988123517\n",
      "Max reward so far:  509186.0\n",
      "Model saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8cc92b2ac28e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#Choose action a from the output probability distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maction_prob_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_distribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mstate_in\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_prob_distribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_prob_distribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "total_rewards = 0\n",
    "maxReward = 0\n",
    "ep = 0\n",
    "ep_states, ep_actions, ep_rewards = [], [], []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for ep in range(max_eps):\n",
    "    ep_reward_sum = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    #env.render()\n",
    "    while True:\n",
    "        \n",
    "        #Choose action a from the output probability distribution\n",
    "        action_prob_distribution = sess.run(action_distribution, feed_dict = {state_in : state.reshape([1, 4])})\n",
    "        action = np.random.choice(range(action_prob_distribution.shape[1]), p = action_prob_distribution.ravel())\n",
    "        \n",
    "        #perform a\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #Store a, s, r\n",
    "        ep_states.append(state)\n",
    "        ep_rewards.append(reward)\n",
    "        \n",
    "        a = np.zeros(act_space)\n",
    "        a[action] = 1\n",
    "        ep_actions.append(a)\n",
    "        \n",
    "        if done:\n",
    "            ep_rewards_sum = np.sum(ep_rewards)\n",
    "            all_rewards.append(ep_rewards_sum)\n",
    "            total_rewards = np.sum(all_rewards)\n",
    "            mean_reward = np.divide(total_rewards, ep + 1)\n",
    "            maxReward = np.amax(all_rewards)\n",
    "            \n",
    "            \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Reward: \", ep_rewards_sum)\n",
    "            print(\"Mean Reward\", mean_reward)\n",
    "            print(\"Max reward so far: \", maxReward)\n",
    "            \n",
    "            #Discounted return\n",
    "            disc_ep_rewards = disc_norm_rewards(ep_rewards)\n",
    "            \n",
    "            _loss, _ = sess.run([loss, opt], feed_dict = {state_in : np.vstack(np.array(ep_states)),\n",
    "                                                          action_in : np.vstack(np.array(ep_actions)),\n",
    "                                                          disc_ep_rewards_in : disc_ep_rewards})\n",
    "                                \n",
    "            ep_states, ep_actions, ep_rewards = [], [], []\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "    #Save model\n",
    "    if ep % 10 == 0:\n",
    "        saver.save(sess, './models/model.ckpt')\n",
    "        print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a7cf05d198>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('episode')\n",
    "plt.ylabel('log( rewards )')\n",
    "plt.title('Cartpole-v0 [Vanilla Policy Gradient]')\n",
    "plt.plot(np.log(np.array(all_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
