{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Add, Multiply\n",
    "#from tensorflow.keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "import random \n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta C}{\\delta \\Theta_A} = \\frac{\\delta C}{\\delta A} \\times \\frac{\\delta A}{\\delta \\Theta_A}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C():\n",
    "    def __init__(self, env, sess):\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.l_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.gamma = 0.95\n",
    "        self.tau = 0.125\n",
    "        self.memory = deque(maxlen = 10000)\n",
    "        \n",
    "        self.actor_state_input, self.actor_model = self.create_actor()\n",
    "        _, self.target_actor_model = self.create_actor()\n",
    "        \n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, shape = [None, self.env.action_space.shape[0]]) #input ùõøùê∂ / ùõøùê¥\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, actor_model_weights, -self.actor_critic_grad) #ùõøùê∂ / ùõøŒò_ùê¥\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.l_rate).apply_gradients(grads)\n",
    "        \n",
    "        self.critic_state_input, self.critic_action_input, self.critic_model = self.create_critic()\n",
    "        _, _, self.target_critic_model = self.create_critic()\n",
    "        \n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) #calculate ùõøùê∂ / ùõøùê¥\n",
    "        \n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "    def create_actor(self):\n",
    "        state_input = Input(shape = self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation = 'relu')(state_input)\n",
    "        h2 = Dense(48, activation = 'relu')(h1)\n",
    "        h3 = Dense(24, activation = 'relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation = 'relu')(h3)\n",
    "        \n",
    "        model = Model(inputs = state_input, outputs = output)\n",
    "        optimizer = Adam(lr = self.l_rate)\n",
    "        model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "        return state_input, model\n",
    "    \n",
    "    def create_critic(self):\n",
    "        state_input = Input(shape = self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation = 'relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "        \n",
    "        action_input = Input(shape = self.env.action_space.shape)\n",
    "        action_h1 = Dense(48)(action_input)\n",
    "        \n",
    "        merged = Add()([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation = 'relu')(merged)\n",
    "        output = Dense(1, activation = 'relu')(merged_h1)\n",
    "        \n",
    "        model = Model(inputs = [state_input, action_input], outputs = output)\n",
    "        optimizer = Adam(lr = self.l_rate)\n",
    "        model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "        return state_input, action_input, model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "        \n",
    "    def train_actor(self, sample):\n",
    "        for state, action, _, _, _ in sample:\n",
    "            pred_action = self.actor_model.predict(state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict = {self.critic_state_input : state, \n",
    "                                                                  self.critic_action_input : pred_action})[0]\n",
    "            \n",
    "            self.sess.run(self.optimize, feed_dict = {self.actor_state_input : state, self.actor_critic_grad : grads})\n",
    "            \n",
    "    def train_critic(self, sample):\n",
    "        for state, action, reward, next_state, done in sample:\n",
    "            value = reward\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(next_state)\n",
    "                future_reward = self.target_critic_model.predict([next_state, target_action])[0][0]\n",
    "                value += self.gamma * future_reward\n",
    "            self.critic_model.fit([state, action], value, verbose = 0)\n",
    "            \n",
    "    def train(self):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        sample = random.sample(self.memory, batch_size)\n",
    "        self.train_critic(sample)\n",
    "        self.train_actor(sample)\n",
    "        \n",
    "    def update_actor_target(self):\n",
    "        actor_model_weights = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_actor_model.get_weights()\n",
    "    \n",
    "        for i in range(len(actor_model_weights)):\n",
    "            actor_target_weights[i] = self.tau * actor_model_weights[i] + (1 - self.tau) * actor_target_weights[i]\n",
    "        self.target_actor_model.set_weights(actor_target_weights)\n",
    "        \n",
    "    def update_critic_target(self):\n",
    "        critic_model_weights = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.target_critic_model.get_weights()\n",
    "        \n",
    "        for i in range(len(critic_model_weights)):\n",
    "            critic_target_weights[i] = self.tau * critic_model_weights[i] + (1 - self.tau) * critic_target_weights[i]\n",
    "        self.target_critic_model.set_weights(critic_target_weights)\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.update_actor_target()\n",
    "        self.update_critic_target()\n",
    "        \n",
    "    def act(self, state):\n",
    "        self.epsilon = max(self.epsilon_decay * self.epsilon , self.epsilon_min)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(state)\n",
    "    \n",
    "    def save_weights(self, path = './model_weights/', filename = '_'):\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        self.actor_model.save_weights(path +'_actor_'+ filename)\n",
    "        self.target_actor_model.save_weights(path +'_target_actor_'+ filename)\n",
    "        self.critic_model.save_weights(path +'critic_'+ filename)\n",
    "        self.target_critic_model.save_weights(path +'_target_critic_actor_'+ filename)\n",
    "        \n",
    "    def load_weights(self, path = './model_weights/', filename = '_'):\n",
    "        \n",
    "        self.actor_model.load_weights(path +'_actor_'+ filename)\n",
    "        self.target_actor_model.load_weights(path +'_target_actor_'+ filename)\n",
    "        self.critic_model.load_weights(path +'critic_'+ filename)\n",
    "        self.target_critic_model.load_weights(path +'_target_critic_actor_'+ filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_infos = {'run' : [], 'total_reward' : [], 'avg_reward' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "       \n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    actor_critic = A3C(env, sess)\n",
    "    \n",
    "    #max_episodes = 10000\n",
    "    max_steps = 500\n",
    "    run = 0\n",
    "    avg_reward = 0\n",
    "    while True:\n",
    "        run_reward = 0\n",
    "        run += 1\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = actor_critic.act(state)\n",
    "            action = np.reshape(action, [1, env.action_space.shape[0]])\n",
    "        \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "            actor_critic.remember(state, action, reward, next_state, done)\n",
    "            actor_critic.train()\n",
    "            actor_critic.update_target()\n",
    "        \n",
    "            state = next_state\n",
    "            run_reward += reward\n",
    "            \n",
    "            #env.render()\n",
    "        \n",
    "        if run == 1:\n",
    "            avg_reward = run_reward\n",
    "        avg_reward = 0.98 * avg_reward + 0.02 * run_reward\n",
    "        \n",
    "        episode_infos['run'].append(run)\n",
    "        episode_infos['total_reward'].append(run_reward)\n",
    "        episode_infos['avg_reward'].append(avg_reward)\n",
    "                \n",
    "        print('epsilon : ' + str(actor_critic.epsilon))\n",
    "        print('run : ' + str(run) + ' score : ' + str(run_reward) + ' avg_score : ' + str(avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\L3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From E:\\Users\\L3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From E:\\Users\\L3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epsilon : 0.7787520933134615\n",
      "run : 1 score : [-2833.3909] avg_score : [-2833.3909]\n",
      "epsilon : 0.606454822840097\n",
      "run : 2 score : [-2253.334] avg_score : [-2821.7898]\n",
      "epsilon : 0.4722779627867691\n",
      "run : 3 score : [-2334.2202] avg_score : [-2812.0383]\n",
      "epsilon : 0.3677874521460121\n",
      "run : 4 score : [-4588.4077] avg_score : [-2847.5657]\n",
      "epsilon : 0.28641524825313086\n",
      "run : 5 score : [-4198.906] avg_score : [-2874.5925]\n",
      "epsilon : 0.22304647413401948\n",
      "run : 6 score : [-4608.947] avg_score : [-2909.2798]\n",
      "epsilon : 0.17369790863805412\n",
      "run : 7 score : [-4288.5117] avg_score : [-2936.8645]\n",
      "epsilon : 0.13526760995605422\n",
      "run : 8 score : [-4308.6074] avg_score : [-2964.2993]\n",
      "epsilon : 0.10533993441078586\n",
      "run : 9 score : [-3552.119] avg_score : [-2976.056]\n",
      "epsilon : 0.0820336944319021\n",
      "run : 10 score : [-4601.8477] avg_score : [-3008.5718]\n",
      "epsilon : 0.06388391126108031\n",
      "run : 11 score : [-4602.3794] avg_score : [-3040.448]\n",
      "epsilon : 0.04974972962361759\n",
      "run : 12 score : [-4586.4395] avg_score : [-3071.368]\n",
      "epsilon : 0.03874270608617077\n",
      "run : 13 score : [-4467.5015] avg_score : [-3099.2908]\n",
      "epsilon : 0.03017096346523357\n",
      "run : 14 score : [-4544.4224] avg_score : [-3128.1934]\n",
      "epsilon : 0.02349570095583451\n",
      "run : 15 score : [-3784.2915] avg_score : [-3141.3154]\n",
      "epsilon : 0.018297326303223154\n",
      "run : 16 score : [-3780.5657] avg_score : [-3154.1006]\n",
      "epsilon : 0.014249081160674432\n",
      "run : 17 score : [-3816.9902] avg_score : [-3167.3584]\n",
      "epsilon : 0.011096501781668586\n",
      "run : 18 score : [-4549.5713] avg_score : [-3195.0027]\n",
      "epsilon : 0.008641423990930954\n",
      "run : 19 score : [-4577.7856] avg_score : [-3222.6584]\n",
      "epsilon : 0.006729527022146624\n",
      "run : 20 score : [-4464.213] avg_score : [-3247.4895]\n",
      "epsilon : 0.005240633255506169\n",
      "run : 21 score : [-4438.29] avg_score : [-3271.3057]\n",
      "epsilon : 0.004081154118013558\n",
      "run : 22 score : [-4516.228] avg_score : [-3296.204]\n",
      "epsilon : 0.003178207312537904\n",
      "run : 23 score : [-4439.183] avg_score : [-3319.0637]\n",
      "epsilon : 0.002475035597623038\n",
      "run : 24 score : [-4555.35] avg_score : [-3343.7896]\n",
      "epsilon : 0.0019274391526742708\n",
      "run : 25 score : [-3839.3916] avg_score : [-3353.7017]\n",
      "epsilon : 0.0015009972748794113\n",
      "run : 26 score : [-4226.0645] avg_score : [-3371.149]\n",
      "epsilon : 0.0011689047698701396\n",
      "run : 27 score : [-3825.8352] avg_score : [-3380.2427]\n",
      "epsilon : 0.0009102870364204561\n",
      "run : 28 score : [-4418.3564] avg_score : [-3401.0051]\n",
      "epsilon : 0.0007088879351285356\n",
      "run : 29 score : [-3822.2375] avg_score : [-3409.43]\n",
      "epsilon : 0.0005520479634060028\n",
      "run : 30 score : [-4535.4155] avg_score : [-3431.9497]\n",
      "epsilon : 0.00042990850711185615\n",
      "run : 31 score : [-3756.3198] avg_score : [-3438.4373]\n",
      "epsilon : 0.0003347921498466223\n",
      "run : 32 score : [-4324.7373] avg_score : [-3456.163]\n",
      "epsilon : 0.00026072008751797056\n",
      "run : 33 score : [-4585.2935] avg_score : [-3478.7456]\n",
      "epsilon : 0.00020303631392348803\n",
      "run : 34 score : [-4483.216] avg_score : [-3498.835]\n",
      "epsilon : 0.0001581149544865652\n",
      "run : 35 score : [-3829.9478] avg_score : [-3505.4573]\n",
      "epsilon : 0.00012313235179057478\n",
      "run : 36 score : [-4501.079] avg_score : [-3525.3696]\n",
      "epsilon : 0.0001\n",
      "run : 37 score : [-4581.153] avg_score : [-3546.4854]\n",
      "epsilon : 0.0001\n",
      "run : 38 score : [-3738.1099] avg_score : [-3550.3179]\n",
      "epsilon : 0.0001\n",
      "run : 39 score : [-3827.857] avg_score : [-3555.8687]\n",
      "epsilon : 0.0001\n",
      "run : 40 score : [-3836.6978] avg_score : [-3561.4854]\n",
      "epsilon : 0.0001\n",
      "run : 41 score : [-4436.173] avg_score : [-3578.979]\n",
      "epsilon : 0.0001\n",
      "run : 42 score : [-4562.728] avg_score : [-3598.654]\n",
      "epsilon : 0.0001\n",
      "run : 43 score : [-4405.749] avg_score : [-3614.7961]\n",
      "epsilon : 0.0001\n",
      "run : 44 score : [-3812.3904] avg_score : [-3618.748]\n",
      "epsilon : 0.0001\n",
      "run : 45 score : [-4386.809] avg_score : [-3634.1091]\n",
      "epsilon : 0.0001\n",
      "run : 46 score : [-4359.8906] avg_score : [-3648.6248]\n",
      "epsilon : 0.0001\n",
      "run : 47 score : [-3822.3083] avg_score : [-3652.0986]\n",
      "epsilon : 0.0001\n",
      "run : 48 score : [-4391.4165] avg_score : [-3666.885]\n",
      "epsilon : 0.0001\n",
      "run : 49 score : [-4363.258] avg_score : [-3680.8125]\n",
      "epsilon : 0.0001\n",
      "run : 50 score : [-4363.3867] avg_score : [-3694.464]\n",
      "epsilon : 0.0001\n",
      "run : 51 score : [-4525.51] avg_score : [-3711.0852]\n",
      "epsilon : 0.0001\n",
      "run : 52 score : [-3834.2239] avg_score : [-3713.548]\n",
      "epsilon : 0.0001\n",
      "run : 53 score : [-4429.072] avg_score : [-3727.8586]\n",
      "epsilon : 0.0001\n",
      "run : 54 score : [-3633.8838] avg_score : [-3725.9792]\n",
      "epsilon : 0.0001\n",
      "run : 55 score : [-3805.444] avg_score : [-3727.5686]\n",
      "epsilon : 0.0001\n",
      "run : 56 score : [-4257.118] avg_score : [-3738.1597]\n",
      "epsilon : 0.0001\n",
      "run : 57 score : [-3824.8994] avg_score : [-3739.8945]\n",
      "epsilon : 0.0001\n",
      "run : 58 score : [-4394.586] avg_score : [-3752.9883]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-dc7d0dc11a87>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mactor_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mactor_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mactor_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-cd4e5c7522a0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_actor_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-cd4e5c7522a0>\u001b[0m in \u001b[0;36mtrain_actor\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mpred_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             grads = self.sess.run(self.critic_grads, feed_dict = {self.critic_state_input : state, \n\u001b[1;32m---> 68\u001b[1;33m                                                                   self.critic_action_input : pred_action})[0]\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_state_input\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_critic_grad\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_r = np.array(episode_infos['total_reward']).reshape([1,episode_infos['run'][-1]])[0]\n",
    "a_r = np.array(episode_infos['avg_reward']).reshape([1,episode_infos['run'][-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "random_x = np.array(range(episode_infos['run'][-1]))#episode_infos['run'][-1]np.linspace(0, 1, episode_infos['run'][-1])\n",
    "\n",
    "# Create a trace\n",
    "reward_total = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = t_r,\n",
    "    line = dict(\n",
    "        color = ('rgb(255, 125, 33)'),\n",
    "        width = 1,)\n",
    ")\n",
    "\n",
    "reward_100_avg_ = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = a_r,\n",
    "    line = dict(\n",
    "        color = ('rgb(66, 134, 244)'),\n",
    "        width = 1,\n",
    "        dash = 'dash')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot([reward_total, reward_100_avg_], filename='basic-area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
