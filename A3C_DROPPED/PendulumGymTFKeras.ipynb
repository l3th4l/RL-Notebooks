{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Add, Multiply\n",
    "#from tensorflow.keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "import random \n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta C}{\\delta \\Theta_A} = \\frac{\\delta C}{\\delta A} \\times \\frac{\\delta A}{\\delta \\Theta_A}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C():\n",
    "    def __init__(self, env, sess):\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.l_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.gamma = 0.95\n",
    "        self.tau = 0.125\n",
    "        self.memory = deque(maxlen = 10000)\n",
    "        \n",
    "        self.actor_state_input, self.actor_model = self.create_actor()\n",
    "        _, self.target_actor_model = self.create_actor()\n",
    "        \n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, shape = [None, self.env.action_space.shape[0]]) #input 𝛿𝐶 / 𝛿𝐴\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, actor_model_weights, -self.actor_critic_grad) #𝛿𝐶 / 𝛿Θ_𝐴\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.l_rate).apply_gradients(grads)\n",
    "        \n",
    "        self.critic_state_input, self.critic_action_input, self.critic_model = self.create_critic()\n",
    "        _, _, self.target_critic_model = self.create_critic()\n",
    "        \n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) #calculate 𝛿𝐶 / 𝛿𝐴\n",
    "        \n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "    def create_actor(self):\n",
    "        state_input = Input(shape = self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation = 'relu')(state_input)\n",
    "        h2 = Dense(48, activation = 'relu')(h1)\n",
    "        h3 = Dense(24, activation = 'relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation = 'relu')(h3)\n",
    "        \n",
    "        model = Model(inputs = state_input, outputs = output)\n",
    "        optimizer = Adam(lr = self.l_rate)\n",
    "        model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "        return state_input, model\n",
    "    \n",
    "    def create_critic(self):\n",
    "        state_input = Input(shape = self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation = 'relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "        \n",
    "        action_input = Input(shape = self.env.action_space.shape)\n",
    "        action_h1 = Dense(48)(action_input)\n",
    "        \n",
    "        merged = Add()([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation = 'relu')(merged)\n",
    "        output = Dense(1, activation = 'relu')(merged_h1)\n",
    "        \n",
    "        model = Model(inputs = [state_input, action_input], outputs = output)\n",
    "        optimizer = Adam(lr = self.l_rate)\n",
    "        model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "        return state_input, action_input, model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "        \n",
    "    def train_actor(self, sample):\n",
    "        for state, action, _, _, _ in sample:\n",
    "            pred_action = self.actor_model.predict(state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict = {self.critic_state_input : state, \n",
    "                                                                  self.critic_action_input : pred_action})[0]\n",
    "            \n",
    "            self.sess.run(self.optimize, feed_dict = {self.actor_state_input : state, self.actor_critic_grad : grads})\n",
    "            \n",
    "    def train_critic(self, sample):\n",
    "        for state, action, reward, next_state, done in sample:\n",
    "            value = reward\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(next_state)\n",
    "                future_reward = self.target_critic_model.predict([next_state, target_action])[0][0]\n",
    "                value += self.gamma * future_reward\n",
    "            self.critic_model.fit([state, action], value, verbose = 0)\n",
    "            \n",
    "    def train(self):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        sample = random.sample(self.memory, batch_size)\n",
    "        self.train_critic(sample)\n",
    "        self.train_actor(sample)\n",
    "        \n",
    "    def update_actor_target(self):\n",
    "        actor_model_weights = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_actor_model.get_weights()\n",
    "    \n",
    "        for i in range(len(actor_model_weights)):\n",
    "            actor_target_weights[i] = self.tau * actor_model_weights[i] + (1 - self.tau) * actor_target_weights[i]\n",
    "        self.target_actor_model.set_weights(actor_target_weights)\n",
    "        \n",
    "    def update_critic_target(self):\n",
    "        critic_model_weights = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.target_critic_model.get_weights()\n",
    "        \n",
    "        for i in range(len(critic_model_weights)):\n",
    "            critic_target_weights[i] = self.tau * critic_model_weights[i] + (1 - self.tau) * critic_target_weights[i]\n",
    "        self.target_critic_model.set_weights(critic_target_weights)\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.update_actor_target()\n",
    "        self.update_critic_target()\n",
    "        \n",
    "    def act(self, state):\n",
    "        self.epsilon = max(self.epsilon_decay * self.epsilon , self.epsilon_min)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(state)\n",
    "    \n",
    "    def save_weights(self, path = './model_weights/', filename = '_'):\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        self.actor_model.save_weights(path +'_actor_'+ filename)\n",
    "        self.target_actor_model.save_weights(path +'_target_actor_'+ filename)\n",
    "        self.critic_model.save_weights(path +'critic_'+ filename)\n",
    "        self.target_critic_model.save_weights(path +'_target_critic_actor_'+ filename)\n",
    "        \n",
    "    def load_weights(self, path = './model_weights/', filename = '_'):\n",
    "        \n",
    "        self.actor_model.load_weights(path +'_actor_'+ filename)\n",
    "        self.target_actor_model.load_weights(path +'_target_actor_'+ filename)\n",
    "        self.critic_model.load_weights(path +'critic_'+ filename)\n",
    "        self.target_critic_model.load_weights(path +'_target_critic_actor_'+ filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_infos = {'run' : [], 'total_reward' : [], 'avg_reward' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "       \n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    actor_critic = A3C(env, sess)\n",
    "    \n",
    "    #max_episodes = 10000\n",
    "    max_steps = 500\n",
    "    run = 0\n",
    "    avg_reward = 0\n",
    "    while True:\n",
    "        run_reward = 0\n",
    "        run += 1\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = actor_critic.act(state)\n",
    "            action = np.reshape(action, [1, env.action_space.shape[0]])\n",
    "        \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "            actor_critic.remember(state, action, reward, next_state, done)\n",
    "            actor_critic.train()\n",
    "            actor_critic.update_target()\n",
    "        \n",
    "            state = next_state\n",
    "            run_reward += reward\n",
    "            \n",
    "            #env.render()\n",
    "        \n",
    "        if run == 1:\n",
    "            avg_reward = run_reward\n",
    "        avg_reward = 0.98 * avg_reward + 0.02 * run_reward\n",
    "        \n",
    "        episode_infos['run'].append(run)\n",
    "        episode_infos['total_reward'].append(run_reward)\n",
    "        episode_infos['avg_reward'].append(avg_reward)\n",
    "                \n",
    "        print('epsilon : ' + str(actor_critic.epsilon))\n",
    "        print('run : ' + str(run) + ' score : ' + str(run_reward) + ' avg_score : ' + str(avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\L3\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (1,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-8df3223ab426>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    290\u001b[0m            [5, 6]])\n\u001b[0;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m--> 292\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# a downstream library like 'pandas'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (1,2)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_r = np.array(episode_infos['total_reward']).reshape([1,episode_infos['run'][-1]])[0]\n",
    "a_r = np.array(episode_infos['avg_reward']).reshape([1,episode_infos['run'][-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "random_x = np.array(range(episode_infos['run'][-1]))#episode_infos['run'][-1]np.linspace(0, 1, episode_infos['run'][-1])\n",
    "\n",
    "# Create a trace\n",
    "reward_total = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = t_r,\n",
    "    line = dict(\n",
    "        color = ('rgb(255, 125, 33)'),\n",
    "        width = 1,)\n",
    ")\n",
    "\n",
    "reward_100_avg_ = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = a_r,\n",
    "    line = dict(\n",
    "        color = ('rgb(66, 134, 244)'),\n",
    "        width = 1,\n",
    "        dash = 'dash')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot([reward_total, reward_100_avg_], filename='basic-area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
