{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Users\\L3\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import gym\n",
    "import numpy as np \n",
    "from collections import deque\n",
    "import tensorflow as tf \n",
    "import tflearn as tl \n",
    "from tflearn.activations import sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function : \n",
    "$$L= L_\\pi + \\alpha L_Q + \\beta L_{reg}$$\n",
    "### Policy loss :\n",
    "$$A = (R - Q_{a_i\\sim \\pi}(a_i,s_i; \\theta'_q))$$\n",
    "$$\\frac{\\partial\\,J(\\pi)}{\\partial\\, \\theta'} = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial\\,log\\,\\pi (a_i|s_i;\\theta')}{\\partial\\,\\theta'}\\; A$$\n",
    "$$\\therefore J(\\pi) = \\frac{1}{n}\\sum_{i=1}^{n}log\\,\\pi (a_i|s_i;\\theta')\\; A\\;\\;\\;\\;\\;\\;[\\because A\\;is\\;considered\\;constant]$$\n",
    "‚àµ We want to maximize ùêΩ(ùúã) $$L_\\pi = -J(\\pi)$$\n",
    "### Value loss :\n",
    "$$J(Q) = \\sum_{i=1}^{n}(R - Q_{a_i\\sim \\pi}(a_i,s_i; \\theta'_q))$$\n",
    "$$L_Q = J(Q)$$\n",
    "### Policy entropy :\n",
    "$$H(\\overrightarrow{\\pi(s)})=-\\sum_{i=1}^{n}\\sum_{k=1}^{m} \\pi(s_i)_k\\cdot log\\, \\pi(s_i)_k$$\n",
    "$$L_{reg}=H(\\overrightarrow{\\pi(s)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0.95\n",
    "l_rate = 0.001\n",
    "\n",
    "exp_memory_size = 1000000\n",
    "batch_size = 20\n",
    "\n",
    "exploration_max = 1.0\n",
    "exploration_min = 0.01\n",
    "exploration_decay = 0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A3CSolver():\n",
    "    def __init__(self, observation_space, action_space, sess):\n",
    "        self.sess = sess\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen = exp_memory_size)\n",
    "        \n",
    "        self.state = tl.input_data(shape = [None, observation_space])\n",
    "        self.action = tl.input_data(shape = [None, action_space])\n",
    "        \n",
    "        #Actor ùúã(a_i|s_i;ùúÉ‚Ä≤)\n",
    "        self.actor = build_actor(self.state, action_space)\n",
    "        #Critic Q_a_i‚àºùúã(a_i,s_i;ùúÉ‚Ä≤_q)\n",
    "        self.critic = build_critic(self.state, self.action)\n",
    "        \n",
    "    def build_actor(self, state_input, action_space):\n",
    "        #State input s_i\n",
    "        a_h1 = tl.fully_connected(state_input, 24)        \n",
    "        a_h2 = tl.fully_connected(self.a_h1, 48)        \n",
    "        a_logit = tl.fully_connected(self.a_h2, 1)\n",
    "        return sigmoid(self.a_logit)\n",
    "    \n",
    "    def build_critic(self, state_input, action_input):\n",
    "        #Action input a_i\n",
    "        as_h1 = tl.fully_connected(action_input, 24)\n",
    "        as_h2 = tl.fully_connected(self.as_h1, 48)\n",
    "        #State input s_i\n",
    "        ss_h1 = tl.fully_connected(state_input, 24)\n",
    "        ss_h2 = tl.fully_connected(self.ss_h2, 48)\n",
    "        #Combine state action input \n",
    "        q_h1 = tl.layers.merge_ops.merge([self.as_h2, self.ss_h2], mode = 'elemwise_sum')\n",
    "        return tl.fully_connected(self.q_h1, 1)\n",
    "        \n",
    "        \n",
    "    def network_loss(self, state, action, reward, state_next, done):\n",
    "        l_policy = policy_loss()\n",
    "        l_value = value_loss()\n",
    "        l_p_entropy = policy_entropy()\n",
    "        \n",
    "    def policy_loss(self, state, action, reward, state_next):\n",
    "        #Advantage calculation A = (R-Q_a_i‚àºùúã(a_i,s_i;ùúÉ‚Ä≤_q))\n",
    "        action_next = self.sess.run(self.actor, feed_dict = {self.state : state_next})\n",
    "        R = reward + y * self.sess.run(self.critic, feed_dict = {self.state : state_next, self.action : action_next})\n",
    "        Q = self.sess.run(self.critic, feed_dict = {self.state : state, self.action : action})\n",
    "        A = R - Q\n",
    "        #policy loss L_ùúã = -1/n‚àëlogùúã(a_i|s_i;ùúÉ‚Ä≤)A\n",
    "        return - tf.reduce_mean(tf.reduce_mean(tf.log(self.actor) * A, axis = -1))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
