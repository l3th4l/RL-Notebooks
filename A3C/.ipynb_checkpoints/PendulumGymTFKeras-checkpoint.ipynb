{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Add, Multiply\n",
    "#from tensorflow.keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "import random \n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta C}{\\delta \\Theta_A} = \\frac{\\delta C}{\\delta A} \\times \\frac{\\delta A}{\\delta \\Theta_A}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C():\n",
    "    def __init__(self, env, sess):\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.l_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.gamma = 0.95\n",
    "        self.tau = 0.125\n",
    "        self.memory = deque(maxlen = 10000)\n",
    "        \n",
    "        self.actor_state_input, self.actor_model = self.create_actor()\n",
    "        _, self.target_actor_model = self.create_actor()\n",
    "        \n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, shape = [None, self.env.action_space.shape[0]]) #input 𝛿𝐶 / 𝛿𝐴\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, actor_model_weights, -self.actor_critic_grad) #𝛿𝐶 / 𝛿Θ_𝐴\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.l_rate).apply_gradients(grads)\n",
    "        \n",
    "        self.critic_state_input, self.critic_action_input, self.critic_model = self.create_critic()\n",
    "        _, _, self.target_critic_model = self.create_critic()\n",
    "        \n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) #calculate 𝛿𝐶 / 𝛿𝐴\n",
    "        \n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "    def create_actor(self):\n",
    "        state_input = Input(shape = self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation = 'relu')(state_input)\n",
    "        h2 = Dense(48, activation = 'relu')(h1)\n",
    "        h3 = Dense(24, activation = 'relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation = 'relu')(h3)\n",
    "        \n",
    "        model = Model(inputs = state_input, outputs = output)\n",
    "        optimizer = Adam(lr = self.l_rate)\n",
    "        model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "        return state_input, model\n",
    "    \n",
    "    def create_critic(self):\n",
    "        state_input = Input(shape = self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation = 'relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "        \n",
    "        action_input = Input(shape = self.env.action_space.shape)\n",
    "        action_h1 = Dense(48)(action_input)\n",
    "        \n",
    "        merged = Add()([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation = 'relu')(merged)\n",
    "        output = Dense(1, activation = 'relu')(merged_h1)\n",
    "        \n",
    "        model = Model(inputs = [state_input, action_input], outputs = output)\n",
    "        optimizer = Adam(lr = self.l_rate)\n",
    "        model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "        return state_input, action_input, model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "        \n",
    "    def train_actor(self, sample):\n",
    "        for state, action, _, _, _ in sample:\n",
    "            pred_action = self.actor_model.predict(state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict = {self.critic_state_input : state, \n",
    "                                                                  self.critic_action_input : pred_action})[0]\n",
    "            \n",
    "            self.sess.run(self.optimize, feed_dict = {self.actor_state_input : state, self.actor_critic_grad : grads})\n",
    "            \n",
    "    def train_critic(self, sample):\n",
    "        for state, action, reward, next_state, done in sample:\n",
    "            value = reward\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(next_state)\n",
    "                future_reward = self.target_critic_model.predict([next_state, target_action])[0][0]\n",
    "                value += self.gamma * future_reward\n",
    "            self.critic_model.fit([state, action], value, verbose = 0)\n",
    "            \n",
    "    def train(self):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        sample = random.sample(self.memory, batch_size)\n",
    "        self.train_critic(sample)\n",
    "        self.train_actor(sample)\n",
    "        \n",
    "    def update_actor_target(self):\n",
    "        actor_model_weights = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_actor_model.get_weights()\n",
    "    \n",
    "        for i in range(len(actor_model_weights)):\n",
    "            actor_target_weights[i] = self.tau * actor_model_weights[i]\n",
    "        self.target_actor_model.set_weights(actor_target_weights)\n",
    "        \n",
    "    def update_critic_target(self):\n",
    "        critic_model_weights = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.target_critic_model.get_weights()\n",
    "        \n",
    "        for i in range(len(critic_model_weights)):\n",
    "            critic_target_weights[i] = self.tau * critic_model_weights[i]\n",
    "        self.target_critic_model.set_weights(critic_target_weights)\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.update_actor_target()\n",
    "        self.update_critic_target()\n",
    "        \n",
    "    def act(self, state):\n",
    "        self.epsilon = max(self.epsilon_decay * self.epsilon , self.epsilon_min)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(state)\n",
    "    \n",
    "    def save_weights(self, path = './model_weights/', filename = '_'):\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        self.actor_model.save_weights(path +'_actor_'+ filename)\n",
    "        self.target_actor_model.save_weights(path +'_target_actor_'+ filename)\n",
    "        self.critic_model.save_weights(path +'critic_'+ filename)\n",
    "        self.target_critic_model.save_weights(path +'_target_critic_actor_'+ filename)\n",
    "        \n",
    "    def load_weights(self, path = './model_weights/', filename = '_'):\n",
    "        \n",
    "        self.actor_model.load_weights(path +'_actor_'+ filename)\n",
    "        self.target_actor_model.load_weights(path +'_target_actor_'+ filename)\n",
    "        self.critic_model.load_weights(path +'critic_'+ filename)\n",
    "        self.target_critic_model.load_weights(path +'_target_critic_actor_'+ filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_infos = {'run' : [], 'total_reward' : [], 'avg_reward' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "       \n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    actor_critic = A3C(env, sess)\n",
    "    \n",
    "    #max_episodes = 10000\n",
    "    max_steps = 100\n",
    "    run = 0\n",
    "    avg_reward = 0\n",
    "    while True:\n",
    "        run_reward = 0\n",
    "        run += 1\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = actor_critic.act(state)\n",
    "            action = np.reshape(action, [1, env.action_space.shape[0]])\n",
    "        \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "            actor_critic.remember(state, action, reward, next_state, done)\n",
    "            actor_critic.train()\n",
    "            actor_critic.update_target()\n",
    "        \n",
    "            state = next_state\n",
    "            run_reward += reward\n",
    "        \n",
    "        if run == 1:\n",
    "            avg_reward = run_reward\n",
    "        avg_reward = 0.99 * avg_reward + 0.01 * run_reward\n",
    "        \n",
    "        episode_infos['run'].append(run)\n",
    "        episode_infos['total_reward'].append(run_reward)\n",
    "        episode_infos['avg_reward'].append(avg_reward)\n",
    "                \n",
    "        print('epsilon : ' + str(actor_critic.epsilon))\n",
    "        print('run : ' + str(run) + ' score : ' + str(run_reward) + ' avg_score : ' + str(avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_r = np.array(episode_infos['total_reward']).reshape([1,episode_infos['run'][-1]])[0]\n",
    "a_r = np.array(episode_infos['avg_reward']).reshape([1,episode_infos['run'][-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "random_x = np.array(range(episode_infos['run'][-1]))#episode_infos['run'][-1]np.linspace(0, 1, episode_infos['run'][-1])\n",
    "\n",
    "# Create a trace\n",
    "reward_total = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = t_r,\n",
    "    line = dict(\n",
    "        color = ('rgb(255, 125, 33)'),\n",
    "        width = 1,)\n",
    ")\n",
    "\n",
    "reward_100_avg_ = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = a_r,\n",
    "    line = dict(\n",
    "        color = ('rgb(66, 134, 244)'),\n",
    "        width = 1,\n",
    "        dash = 'dash')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot([reward_total, reward_100_avg_], filename='basic-area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
